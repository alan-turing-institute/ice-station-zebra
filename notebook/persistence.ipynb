{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data\n",
    "\n",
    "Create masks for masking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_01.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_02.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_03.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_04.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_05.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_06.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_07.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_08.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_09.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_10.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_11.npy, already exists\n",
      "INFO:root:Skipping ./data/masks/south/masks/active_grid_cell_mask_12.npy, already exists\n"
     ]
    }
   ],
   "source": [
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea Ice data\n",
    "\n",
    "Download sea ice concentration from OSI-SAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading SIC datafiles to .temp intermediates...\n",
      "INFO:root:Excluding 121 dates already existing from 121 dates requested.\n",
      "INFO:root:Opening for interpolation: ['./data/osisaf/south/siconca/2020.nc']\n",
      "INFO:root:Processing 0 missing dates\n"
     ]
    }
   ],
   "source": [
    "sic = SICDownloader(\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    parallel_opens=False,\n",
    ")\n",
    "\n",
    "sic.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Process downloaded datasets.\n",
    "\n",
    "To make life easier, setting up train, val, test dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-03-31\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-03\", \"2020-04-23\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    ")\n",
    "processed_name = \"notebook_api_pytorch_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor\n",
    "\n",
    "\n",
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],\n",
    "    [],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing 91 dates for train category\n",
      "INFO:root:Including lag of 1 days\n",
      "INFO:root:Including lead of 93 days\n",
      "INFO:root:No data found for 2019-12-31, outside data boundary perhaps?\n",
      "INFO:root:Processing 21 dates for val category\n",
      "INFO:root:Including lag of 1 days\n",
      "INFO:root:Including lead of 93 days\n",
      "INFO:root:Processing 2 dates for test category\n",
      "INFO:root:Including lag of 1 days\n",
      "INFO:root:Including lead of 93 days\n",
      "INFO:root:Got 1 files for siconca\n",
      "INFO:root:Opening files for siconca\n",
      "INFO:root:Filtered to 121 units long based on configuration requirements\n",
      "INFO:root:No normalisation for siconca\n",
      "INFO:root:Loading configuration ./loader.notebook_api_pytorch_data.json\n",
      "INFO:root:Writing configuration to ./loader.notebook_api_pytorch_data.json\n",
      "INFO:root:Loading configuration ./loader.notebook_api_pytorch_data.json\n",
      "INFO:root:Writing configuration to ./loader.notebook_api_pytorch_data.json\n"
     ]
    }
   ],
   "source": [
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "As with the `icenet_dataset_create` command we can create a dataset configuration for training the network. As before this can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration loader.notebook_api_pytorch_data.json\n"
     ]
    }
   ],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_pytorch_data.json\"\n",
    "dataset_name = \"notebook_api_pytorch_data\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=1,\n",
    "    generate_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.notebook_api_pytorch_data.json` (recall we set the dataset name as `notebook_api_pytorch_data` above).\n",
    "\n",
    "In this case, for pytorch, will read data in directly, rather than using cached tfrecords inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing dataset configuration without data generation\n",
      "INFO:root:91 train dates in total, NOT generating cache data.\n",
      "INFO:root:21 val dates in total, NOT generating cache data.\n",
      "INFO:root:2 test dates in total, NOT generating cache data.\n",
      "INFO:root:Writing configuration to ./dataset_config.notebook_api_pytorch_data.json\n"
     ]
    }
   ],
   "source": [
    "dl.write_dataset_config_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the IceNetDataSet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSetPyTorch\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "We implement a custom PyTorch class for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence Model\n",
    "\n",
    "Simple persistence model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "class PersistenceModel:\n",
    "    \"\"\"Simple persistence model: copy the latest frame N times.\"\"\"\n",
    "    def __init__(self, forecast_steps: int = 12):\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        # X shape: (batch, channels, time, height, width)\n",
    "        latest_frame = X[:, :, -1:, :, :]  # shape: (B, C, 1, H, W)\n",
    "\n",
    "        # Repeat the last frame across the forecast steps\n",
    "        y_hat = np.repeat(latest_frame, self.forecast_steps, axis=2)\n",
    "\n",
    "        # Clip to valid range [0, 1]\n",
    "        return y_hat.clip(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration dataset_config.notebook_api_pytorch_data.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /Users/npedrazzini/Desktop/ice-station-zebra/notebook/loader.notebook_api_pytorch_data.json\n"
     ]
    }
   ],
   "source": [
    "train_dataset = IceNetDataSetPyTorch(dataset_config, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic = train_dataset[1][0][:,:,0]  # SIC - channel 0\n",
    "X_input = sic[np.newaxis, np.newaxis, np.newaxis, :, :]  # (1, 1, 1, 432, 432)\n",
    "\n",
    "model = PersistenceModel(forecast_steps=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X_input)  # (1, 1, 12, 432, 432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# forecast = prediction.squeeze(0).squeeze(0)\n",
    "\n",
    "# forecast_start_date = pd.Timestamp(\"2020-09-01\") #random date\n",
    "# time = [forecast_start_date + dt.timedelta(days=int(lead)) for lead in range(forecast.shape[0])]\n",
    "\n",
    "# da = xr.DataArray(\n",
    "#     forecast,\n",
    "#     dims=(\"time\", \"y\", \"x\"),\n",
    "#     coords=dict(\n",
    "#         time=time,\n",
    "#         y=np.arange(forecast.shape[1]),\n",
    "#         x=np.arange(forecast.shape[2]),\n",
    "#         yc=(\"y\", np.arange(forecast.shape[1])),  # add yc coordinate\n",
    "#         xc=(\"x\", np.arange(forecast.shape[2]))   # add xc coordinate\n",
    "#     ),\n",
    "#     name=\"sic_mean\"\n",
    "# )\n",
    "\n",
    "# land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "# print(land_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anim = xvid(da, fps=15, figsize=(4, 4), mask=land_mask)\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seaice_env_min",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
