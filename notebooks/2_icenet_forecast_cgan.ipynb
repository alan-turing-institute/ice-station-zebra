{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.b. this notebook is from the [IceNet notebooks repo](https://github.com/icenet-ai/icenet-notebooks/blob/main/pytorch/2_icenet_forecast_cgan.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch cGAN implementation using IceNet library for data download and post-processing of sea ice forecasting.\n",
    "\n",
    "This notebook has been designed to be independent of other notebooks.\n",
    "\n",
    "### Highlights\n",
    "The key features of this notebook are:\n",
    "* [1. Download](#1.-Download) \n",
    "* [2. Data Processing](#2.-Data-Processing)\n",
    "* [3. Train](#3.-Train)\n",
    "* [4. Prediction](#4.-Prediction)\n",
    "* [5. Outputs and Plotting](#5.-Outputs-and-Plotting)\n",
    "\n",
    "Please note that this notebook relies on a pytorch data loader implementation which is only available from icenet v0.2.8+.\n",
    "\n",
    "To install the necessary python packages, you can use the conda `icenet-notebooks/pytorch/environment.yml` environment file on a Linux system to be able to set-up the necessary pytorch + tensorflow + CUDA + other modules which could be a tricky mix to get working manually:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "### Contributions\n",
    "#### PyTorch implementation of IceNet\n",
    "\n",
    "Andrew McDonald ([icenet-gan](https://github.com/ampersandmcd/icenet-gan))\n",
    "\n",
    "Bryn Noel Ubald (Refactor, updates for daily predictions and matching icenet library)\n",
    "\n",
    "#### Notebook\n",
    "Bryn Noel Ubald (author)\n",
    "\n",
    "#### PyTorch Integration\n",
    "Bryn Noel Ubald\n",
    "\n",
    "Ryan Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base path where data should be stored. This should be the same as what you use in your IceStationZebra config.\n",
    "base_path = \"/LOCAL/PATH/WHERE/YOU/WANT/TO/STORE/DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (Path(base_path) / \"data\" / \"notebooks\" / \"icenet_cgan\").resolve()\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "current_directory = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unset SLURM_NTASKS if it's causing issues\n",
    "if \"SLURM_NTASKS\" in os.environ:\n",
    "    del os.environ[\"SLURM_NTASKS\"]\n",
    "\n",
    "# Optionally, set SLURM_NTASKS_PER_NODE if needed\n",
    "os.environ[\"SLURM_NTASKS_PER_NODE\"] = \"1\"  # or whatever value is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data\n",
    "\n",
    "Create masks for masking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Download Daily Data for IceNet\n",
    "\n",
    ":::note\n",
    "The prototype data currently in use (South Pole, 2020) can be downloaded from **Baskerville** at the following path: `/vjgo8416-ice-frcst/shared/prototype_data/`\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate and Sea Ice data\n",
    "\n",
    "Download climate variables from ERA5 and sea ice concentration from OSI-SAF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ERA5 Data  \n",
    "\n",
    "Use the following link to set up the ERA5 API: [https://cds.climate.copernicus.eu/how-to-api?](https://cds.climate.copernicus.eu/how-to-api?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],  # Name of variables to download\n",
    "    levels=[\n",
    "        None,\n",
    "        [250, 500],\n",
    "        None,\n",
    "        None,\n",
    "    ],  # The levels at which to obtain the variables for (e.g. for zg, it is the pressure levels)\n",
    "    dates=[\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(  # Dates to download the variable data for\n",
    "            \"2020-01-01\", \"2020-04-30\", freq=\"D\"\n",
    "        )\n",
    "    ],\n",
    "    delete_tempfiles=False,  # Whether to delete temporary downloaded files\n",
    "    max_threads=64,  # Maximum number of concurrent downloads\n",
    "    north=False,  # Boolean: Whether require data across northern hemisphere\n",
    "    south=True,  # Boolean: Whether require data across southern hemisphere\n",
    "    # NOTE: there appears to be a bug with the toolbox API at present (icenet#54)\n",
    "    use_toolbox=False,  # Experimental, alternative download method\n",
    ")\n",
    "\n",
    "# era5.download()  # Uncomment this line to download the dataos.chdir(current_directory)\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-grid ERA5 reanalysis data, and rotate wind vector data from ERA5 to align with EASE2 projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "era5.regrid()\n",
    "era5.rotate_wind_data()\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download SIC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_path)\n",
    "sic = SICDownloader(\n",
    "    dates=[\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(  # Dates to download SIC data for\n",
    "            \"2020-01-01\", \"2020-04-30\", freq=\"D\"\n",
    "        )\n",
    "    ],\n",
    "    delete_tempfiles=False,  # Delete temporary downloaded files after use\n",
    "    north=False,  # Use mask for the Northern Hemisphere (set to True if needed)\n",
    "    south=True,  # Use mask for the Southern Hemisphere\n",
    "    parallel_opens=True,  # Enable parallel processing with dask.delayed\n",
    ")\n",
    "\n",
    "sic.download()  # Uncomment this line to download the data\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Process downloaded datasets.\n",
    "\n",
    "To make life easier, setting up train, val, test dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-12-31\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2021-01-01\", \"2021-01-31\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2012-12-01\", \"2019-12-31\")],\n",
    ")\n",
    "processed_name = \"notebook_api_pytorch_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor\n",
    "\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],\n",
    "    [\"tas\", \"zg500\", \"zg250\"],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True,\n",
    ")\n",
    "\n",
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],\n",
    "    [],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True,\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(processed_name, north=False, south=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causes hanging on training, when generating sample.\n",
    "pp.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "pp.process()\n",
    "\n",
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "As with the `icenet_dataset_create` command we can create a dataset configuration for training the network. As before this can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_pytorch_data.json\"\n",
    "dataset_name = \"notebook_api_pytorch_data\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=1,\n",
    "    generate_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.notebook_api_pytorch_data.json` (recall we set the dataset name as `notebook_api_pytorch_data` above).\n",
    "\n",
    "In this case, for pytorch, will read data in directly, rather than using cached tfrecords inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.write_dataset_config_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the IceNetDataSet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSetPyTorch\n",
    "\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shuffle = False\n",
    "# persistent_workers=True\n",
    "# num_workers = 4\n",
    "persistent_workers = True\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "We implement a custom PyTorch class for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IceNet2 GAN model\n",
    "\n",
    "PyTorch UNet implementation based on modifications to Andrew's work from https://github.com/ampersandmcd/icenet-gan/ to match current state of IceNet2 (daily averaging instead of monthly, and binary classification instead of triple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Generator part - it is still UNet based, like the first PyTorch notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super().__init__()\n",
    "        self.interp = F.interpolate\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        filter_size=3,\n",
    "        n_filters_factor=1,\n",
    "        n_forecast_days=7,\n",
    "        n_output_classes=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.n_filters_factor = n_filters_factor\n",
    "        self.n_forecast_days = n_forecast_days\n",
    "        self.n_output_classes = n_output_classes\n",
    "\n",
    "        start_out_channels = 64\n",
    "        reduced_channels = int(start_out_channels * n_filters_factor)\n",
    "        channels = {\n",
    "            start_out_channels * 2**pow: reduced_channels * 2**pow for pow in range(4)\n",
    "        }\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = self.conv_block(input_channels, channels[64])\n",
    "        self.conv2 = self.conv_block(channels[64], channels[128])\n",
    "        self.conv3 = self.conv_block(channels[128], channels[256])\n",
    "        self.conv4 = self.conv_block(channels[256], channels[256])\n",
    "\n",
    "        # Bottleneck\n",
    "        self.conv5 = self.bottleneck_block(channels[256], channels[512])\n",
    "\n",
    "        # Decoder\n",
    "        self.up6 = self.upconv_block(channels[512], channels[256])\n",
    "        self.up7 = self.upconv_block(channels[256], channels[256])\n",
    "        self.up8 = self.upconv_block(channels[256], channels[128])\n",
    "        self.up9 = self.upconv_block(channels[128], channels[64])\n",
    "\n",
    "        self.up6b = self.conv_block(channels[512], channels[256])\n",
    "        self.up7b = self.conv_block(channels[512], channels[256])\n",
    "        self.up8b = self.conv_block(channels[256], channels[128])\n",
    "        self.up9b = self.conv_block(channels[128], channels[64], final=True)\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Conv2d(\n",
    "            channels[64], n_forecast_days, kernel_size=1, padding=\"same\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transpose from shape (b, h, w, c) to (b, c, h, w) for pytorch conv2d layers\n",
    "        x = torch.movedim(x, -1, 1)  # move c from last to second dim\n",
    "\n",
    "        # Encoder\n",
    "        bn1 = self.conv1(x)\n",
    "        conv1 = F.max_pool2d(bn1, kernel_size=2)\n",
    "        bn2 = self.conv2(conv1)\n",
    "        conv2 = F.max_pool2d(bn2, kernel_size=2)\n",
    "        bn3 = self.conv3(conv2)\n",
    "        conv3 = F.max_pool2d(bn3, kernel_size=2)\n",
    "        bn4 = self.conv4(conv3)\n",
    "        conv4 = F.max_pool2d(bn4, kernel_size=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        bn5 = self.conv5(conv4)\n",
    "\n",
    "        # Decoder\n",
    "        up6 = self.up6b(torch.cat([bn4, self.up6(bn5)], dim=1))\n",
    "        up7 = self.up7b(torch.cat([bn3, self.up7(up6)], dim=1))\n",
    "        up8 = self.up8b(torch.cat([bn2, self.up8(up7)], dim=1))\n",
    "        up9 = self.up9b(torch.cat([bn1, self.up9(up8)], dim=1))\n",
    "\n",
    "        # Final layer\n",
    "        output = self.final_layer(up9)\n",
    "\n",
    "        # transpose from shape (b, c, h, w) back to (b, h, w, c) to align with training data\n",
    "        output = torch.movedim(output, 1, -1)  # move c from second to final dim\n",
    "\n",
    "        b, h, w, c = output.shape\n",
    "\n",
    "        # unpack c=classes*months dimension into classes, months as separate dimensions\n",
    "        output = output.reshape((b, h, w, self.n_output_classes, self.n_forecast_days))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, final=False):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=self.filter_size, padding=\"same\"\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=self.filter_size, padding=\"same\"\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        if not final:\n",
    "            batch_norm = nn.Sequential(\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "            )\n",
    "            return nn.Sequential().extend(block).extend(batch_norm)\n",
    "        else:\n",
    "            final_block = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=self.filter_size,\n",
    "                    padding=\"same\",\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            return nn.Sequential().extend(block).extend(final_block)\n",
    "\n",
    "    def bottleneck_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=self.filter_size, padding=\"same\"\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=self.filter_size, padding=\"same\"\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            Interpolate(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=2, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(UNet):\n",
    "    \"\"\"\n",
    "    An implementation of a conditional GAN which combines predictors and noise in a UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        filter_size=3,\n",
    "        n_filters_factor=1,\n",
    "        n_forecast_days=7,\n",
    "        n_output_classes=1,\n",
    "        sigma=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs underlying UNet and adds some additional GAN-specific logic.\n",
    "        :param seed: Seed for the generator's underlying random noise generator\n",
    "        :param sigma: Standard deviation parameter for underlying random noise generator.\n",
    "            In theory, higher values will lead to higher-temperature samples, with a more\n",
    "            noticeable impact on the resulting generated image.\n",
    "        \"\"\"\n",
    "\n",
    "        # construct UNet\n",
    "        super(Generator, self).__init__(\n",
    "            input_channels + 1,\n",
    "            filter_size,\n",
    "            n_filters_factor,\n",
    "            n_forecast_days,\n",
    "            n_output_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # set sigma\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        # sample noise in the shape of the image\n",
    "        # seed self.generator before this step if you want determinism\n",
    "        b, h, w, c = x.shape\n",
    "        if noise is None:\n",
    "            noise = torch.randn(b, h, w, 1, device=x.device) * self.sigma\n",
    "\n",
    "        # concatenate noise onto channel dimension\n",
    "        x = torch.cat([x, noise], dim=-1)\n",
    "\n",
    "        # pass noisified input to the unet\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the Discriminator part of the cGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of a discriminator for detecting real/fake images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        filter_size=3,\n",
    "        n_filters_factor=1,\n",
    "        n_forecast_days=7,\n",
    "        mode=\"forecast\",\n",
    "        n_output_classes=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor parameters are a bit illogical to maintain API consistency with other networks\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_channels = (\n",
    "            input_channels  # not used but maintained for API continuity\n",
    "        )\n",
    "        self.filter_size = filter_size\n",
    "        self.n_filters_factor = n_filters_factor\n",
    "        self.n_forecast_days = n_forecast_days\n",
    "        self.mode = mode\n",
    "        self.n_output_classes = n_output_classes\n",
    "\n",
    "        if mode == \"forecast\":  # evaluate entire forecast at once\n",
    "            in_channels = n_forecast_days * n_output_classes\n",
    "        elif mode == \"onestep\":  # evaluate one step at a time\n",
    "            in_channels = n_output_classes\n",
    "\n",
    "        start_out_channels = 64\n",
    "        reduced_channels = int(start_out_channels * n_filters_factor)\n",
    "        channels = {\n",
    "            start_out_channels * 2**pow: reduced_channels * 2**pow for pow in range(4)\n",
    "        }\n",
    "\n",
    "        self.conv1a = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=channels[64],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv1b = nn.Conv2d(\n",
    "            in_channels=channels[64],\n",
    "            out_channels=channels[64],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=channels[64])\n",
    "\n",
    "        self.conv2a = nn.Conv2d(\n",
    "            in_channels=channels[64],\n",
    "            out_channels=channels[128],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2b = nn.Conv2d(\n",
    "            in_channels=channels[128],\n",
    "            out_channels=channels[128],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=channels[128])\n",
    "\n",
    "        self.conv3a = nn.Conv2d(\n",
    "            in_channels=channels[128],\n",
    "            out_channels=channels[256],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv3b = nn.Conv2d(\n",
    "            in_channels=channels[256],\n",
    "            out_channels=channels[256],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=channels[256])\n",
    "\n",
    "        self.conv4a = nn.Conv2d(\n",
    "            in_channels=channels[256],\n",
    "            out_channels=channels[256],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv4b = nn.Conv2d(\n",
    "            in_channels=channels[256],\n",
    "            out_channels=channels[256],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=channels[256])\n",
    "\n",
    "        self.conv5a = nn.Conv2d(\n",
    "            in_channels=channels[256],\n",
    "            out_channels=channels[512],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv5b = nn.Conv2d(\n",
    "            in_channels=channels[512],\n",
    "            out_channels=channels[512],\n",
    "            kernel_size=filter_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "        # classify as real/fake\n",
    "        self.fc = nn.Linear(in_features=channels[512], out_features=1)\n",
    "\n",
    "    def forward(self, x, sample_weight):\n",
    "        # our discriminator should only pay attention to pixels where sample_weight > 0\n",
    "        x = x * sample_weight\n",
    "\n",
    "        # reshape depending on forecast mode\n",
    "        b, h, w, c, t = x.shape\n",
    "        if self.mode == \"forecast\":\n",
    "            # combine c and t into one dimension for per-forecast classification\n",
    "            x = x.reshape((b, h, w, c * t))\n",
    "        elif self.mode == \"onestep\":\n",
    "            x = torch.movedim(x, 0, -1)  # place batch dimension last\n",
    "            x = x.reshape(\n",
    "                (h, w, c, b * t)\n",
    "            )  # combine b and t such that each t is its own instance\n",
    "            x = torch.movedim(x, -1, 0)  # place batch dimension first again\n",
    "\n",
    "        # now transpose from shape (b, h, w, c) to (b, c, h, w) for pytorch conv2d layers\n",
    "        x = torch.movedim(x, -1, 1)  # move c from last to second dim\n",
    "\n",
    "        # run through network\n",
    "        conv1 = self.conv1a(x)  # input to 64\n",
    "        conv1 = F.relu(conv1)\n",
    "        conv1 = self.conv1b(conv1)  # 64 to 64\n",
    "        conv1 = F.relu(conv1)\n",
    "        bn1 = self.bn1(conv1)\n",
    "        pool1 = F.max_pool2d(bn1, kernel_size=(2, 2))\n",
    "\n",
    "        conv2 = self.conv2a(pool1)  # 64 to 128\n",
    "        conv2 = F.relu(conv2)\n",
    "        conv2 = self.conv2b(conv2)  # 128 to 128\n",
    "        conv2 = F.relu(conv2)\n",
    "        bn2 = self.bn2(conv2)\n",
    "        pool2 = F.max_pool2d(bn2, kernel_size=(2, 2))\n",
    "\n",
    "        conv3 = self.conv3a(pool2)  # 128 to 256\n",
    "        conv3 = F.relu(conv3)\n",
    "        conv3 = self.conv3b(conv3)  # 256 to 256\n",
    "        conv3 = F.relu(conv3)\n",
    "        bn3 = self.bn3(conv3)\n",
    "        pool3 = F.max_pool2d(bn3, kernel_size=(2, 2))\n",
    "\n",
    "        conv4 = self.conv4a(pool3)  # 256 to 256\n",
    "        conv4 = F.relu(conv4)\n",
    "        conv4 = self.conv4b(conv4)  # 256 to 256\n",
    "        conv4 = F.relu(conv4)\n",
    "        bn4 = self.bn4(conv4)\n",
    "        pool4 = F.max_pool2d(bn4, kernel_size=(2, 2))\n",
    "\n",
    "        conv5 = self.conv5a(pool4)  # 256 to 512\n",
    "        conv5 = F.relu(conv5)\n",
    "        conv5 = self.conv5b(conv5)  # 512 to 512\n",
    "        conv5 = F.relu(conv5)  # no batch norm on last layer\n",
    "\n",
    "        pool5 = self.avgpool(conv5)  # shape (b, 512, 1, 1)\n",
    "        pool5 = pool5.squeeze(-1, -2)  # shape (b, 512)\n",
    "\n",
    "        logits = self.fc(pool5)  # shape (b, 1)\n",
    "        return logits  # return logits of p(real) (apply softmax to get p(real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom metrics for use in validation and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "class IceNetAccuracy(Metric):\n",
    "    \"\"\"Binary accuracy metric for use at multiple leadtimes.\n",
    "\n",
    "    Reference: https://lightning.ai/docs/torchmetrics/stable/pages/implement.html\n",
    "    \"\"\"\n",
    "\n",
    "    # Set class properties\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = True\n",
    "    full_state_update: bool = True\n",
    "\n",
    "    def __init__(self, leadtimes_to_evaluate: list):\n",
    "        \"\"\"Custom loss/metric for binary accuracy in classifying SIC>15% for multiple leadtimes.\n",
    "\n",
    "        Args:\n",
    "            leadtimes_to_evaluate: A list of leadtimes to consider\n",
    "                e.g., [0, 1, 2, 3, 4, 5] to consider first six days in accuracy computation or\n",
    "                e.g., [0] to only look at the first day's accuracy\n",
    "                e.g., [5] to only look at the sixth day's accuracy\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leadtimes_to_evaluate = leadtimes_to_evaluate\n",
    "        self.add_state(\n",
    "            \"weighted_score\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\"\n",
    "        )\n",
    "        self.add_state(\n",
    "            \"possible_score\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\"\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self, preds: torch.Tensor, target: torch.Tensor, sample_weight: torch.Tensor\n",
    "    ):\n",
    "        # preds and target are shape (b, h, w, t)\n",
    "        preds = (preds > 0.15).long()  # torch.Size([2, 432, 432, 7])\n",
    "        target = (target > 0.15).long()  # torch.Size([2, 432, 432, 7])\n",
    "        base_score = (\n",
    "            preds[:, :, :, self.leadtimes_to_evaluate]\n",
    "            == target[:, :, :, self.leadtimes_to_evaluate]\n",
    "        )\n",
    "        self.weighted_score += torch.sum(\n",
    "            base_score * sample_weight[:, :, :, self.leadtimes_to_evaluate]\n",
    "        )\n",
    "        self.possible_score += torch.sum(\n",
    "            sample_weight[:, :, :, self.leadtimes_to_evaluate]\n",
    "        )\n",
    "\n",
    "    def compute(self):\n",
    "        return self.weighted_score.float() / self.possible_score * 100.0\n",
    "\n",
    "\n",
    "class SIEError(Metric):\n",
    "    \"\"\"\n",
    "    Sea Ice Extent error metric (in km^2) for use at multiple leadtimes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set class properties\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = False\n",
    "    full_state_update: bool = True\n",
    "\n",
    "    def __init__(self, leadtimes_to_evaluate: list):\n",
    "        \"\"\"Construct an SIE error metric (in km^2) for use at multiple leadtimes.\n",
    "        leadtimes_to_evaluate: A list of leadtimes to consider\n",
    "            e.g., [0, 1, 2, 3, 4, 5] to consider six days in computation or\n",
    "            e.g., [0] to only look at the first day\n",
    "            e.g., [5] to only look at the sixth day\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leadtimes_to_evaluate = leadtimes_to_evaluate\n",
    "        self.add_state(\"pred_sie\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"true_sie\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(\n",
    "        self, preds: torch.Tensor, target: torch.Tensor, sample_weight: torch.Tensor\n",
    "    ):\n",
    "        # preds and target are shape (b, h, w, t)\n",
    "        preds = (preds > 0.15).long()\n",
    "        target = (target > 0.15).long()\n",
    "        self.pred_sie += preds[:, :, :, self.leadtimes_to_evaluate].sum()\n",
    "        self.true_sie += target[:, :, :, self.leadtimes_to_evaluate].sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return (self.pred_sie - self.true_sie) * 25**2  # each pixel is 25x25 km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCEWithLogitsLoss(nn.BCEWithLogitsLoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets, sample_weights):\n",
    "        \"\"\"\n",
    "        Weighted BCEWithLogitsLoss loss.\n",
    "\n",
    "        Compute BCEWithLogitsLoss loss weighted by masking.\n",
    "\n",
    "        Using BCEWithLogitsLoss instead of BCELoss, as pytorch docs mentions it is\n",
    "        more numerically stable.\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "        \"\"\"\n",
    "        # Computing using nn.BCEWithLogitsLoss base class. This class must be instantiated via:\n",
    "        # >>> criterion = WeightedBCEWithLogitsLoss(reduction='none')\n",
    "        loss = super().forward(\n",
    "            (inputs.movedim(-2, 1)), (targets.movedim(-1, 1))\n",
    "        ) * sample_weights.movedim(-1, 1)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class WeightedL1Loss(nn.L1Loss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets, sample_weights):\n",
    "        \"\"\"\n",
    "        Weighted L1 loss.\n",
    "\n",
    "        Compute L1 loss weighted by masking.\n",
    "\n",
    "        \"\"\"\n",
    "        y_hat = torch.sigmoid(inputs)\n",
    "\n",
    "        # Computing using nn.L1Loss class. This class must be instantiated via:\n",
    "        # >>> criterion = WeightedL1Loss(reduction=\"none\")\n",
    "        loss = super().forward(\n",
    "            (100 * y_hat.movedim(-2, 1)), (100 * targets.movedim(-1, 1))\n",
    "        ) * sample_weights.movedim(-1, 1)\n",
    "\n",
    "        # Computing here, in the derived class\n",
    "        # loss = (\n",
    "        #             torch.abs( ( y_hat.movedim(-2, 1) - targets.movedim(-1, 1) )*100 )\n",
    "        #         )*sample_weights.movedim(-1, 1)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class WeightedMSELoss(nn.MSELoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs, targets, sample_weights):\n",
    "        \"\"\"\n",
    "        Weighted MSE loss.\n",
    "\n",
    "        Compute MSE loss weighted by masking.\n",
    "\n",
    "        \"\"\"\n",
    "        y_hat = torch.sigmoid(inputs)\n",
    "\n",
    "        # Computing using nn.MSELoss base class. This class must be instantiated via:\n",
    "        # criterion = nn.MSELoss(reduction=\"none\")\n",
    "        loss = super().forward(\n",
    "            (100 * y_hat.movedim(-2, 1)), (100 * targets.movedim(-1, 1))\n",
    "        ) * sample_weights.movedim(-1, 1)\n",
    "\n",
    "        # Computing here, in the nn.Module derived class\n",
    "        # loss = (\n",
    "        #             ( ( y_hat.movedim(-2, 1) - targets.movedim(-1, 1) )*100 )**2\n",
    "        #         )*sample_weights.movedim(-1, 1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _LightningModule_ wrapper for the GAN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "\n",
    "class LitGAN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A LightningModule wrapping the GAN implementation of IceNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        generator_fake_criterion: callable,\n",
    "        generator_structural_criterion: callable,\n",
    "        generator_lambda: float,\n",
    "        discriminator_criterion: callable,\n",
    "        learning_rate: float,\n",
    "        d_lr_factor: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Construct a UNet LightningModule.\n",
    "        Note that we keep hyperparameters separate from dataloaders to prevent data leakage at test time.\n",
    "        :param generator: PyTorch model to generate forecasts\n",
    "        :param discriminator: PyTorch model to discriminate between real forecasts (observations) and fake forecasts\n",
    "        :param generator_fake_criterion: Instance-level loss function for G instantiated with reduction=\"none\"\n",
    "        :param generator_structural_criterion: Structural-level loss function for G instantiated with reduction=\"none\"\n",
    "        :param generator_lambda: Parameter to trade off between instance and structure loss for G\n",
    "        :param discriminator_criterion: Instance-level loss function for D instantiated with reduction=\"none\"\n",
    "        :param learning_rate: Float learning rate for our optimiser\n",
    "        :param d_lr_factor: Float factor to adjust D learning rate so it is balanced with G\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.generator_fake_criterion = generator_fake_criterion\n",
    "        self.generator_structural_criterion = generator_structural_criterion\n",
    "        self.generator_lambda = generator_lambda\n",
    "        self.discriminator_criterion = discriminator_criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.d_lr_factor = d_lr_factor\n",
    "        self.n_output_classes = (\n",
    "            generator.n_output_classes\n",
    "        )  # this should be a property of the network\n",
    "\n",
    "        # manually control optimisation\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # evaluation metrics\n",
    "        metrics = {\n",
    "            \"val_accuracy\": IceNetAccuracy(\n",
    "                leadtimes_to_evaluate=list(range(self.generator.n_forecast_days))\n",
    "            ),\n",
    "            \"val_sieerror\": SIEError(\n",
    "                leadtimes_to_evaluate=list(range(self.generator.n_forecast_days))\n",
    "            ),\n",
    "        }\n",
    "        for i in range(self.generator.n_forecast_days):\n",
    "            metrics[f\"val_accuracy_{i}\"] = IceNetAccuracy(leadtimes_to_evaluate=[i])\n",
    "            metrics[f\"val_sieerror_{i}\"] = SIEError(leadtimes_to_evaluate=[i])\n",
    "        self.metrics = MetricCollection(metrics)\n",
    "\n",
    "        test_metrics = {\n",
    "            \"test_accuracy\": IceNetAccuracy(\n",
    "                leadtimes_to_evaluate=list(range(self.generator.n_forecast_days))\n",
    "            ),\n",
    "            \"test_sieerror\": SIEError(\n",
    "                leadtimes_to_evaluate=list(range(self.generator.n_forecast_days))\n",
    "            ),\n",
    "        }\n",
    "        for i in range(self.generator.n_forecast_days):\n",
    "            test_metrics[f\"test_accuracy_{i}\"] = IceNetAccuracy(\n",
    "                leadtimes_to_evaluate=[i]\n",
    "            )\n",
    "            test_metrics[f\"test_sieerror_{i}\"] = SIEError(leadtimes_to_evaluate=[i])\n",
    "        self.test_metrics = MetricCollection(test_metrics)\n",
    "\n",
    "        # Save input parameters to __init__ (hyperparams) when checkpointing.\n",
    "        # self.save_hyperparameters(ignore=[\"generator\", \"discriminator\",\n",
    "        #                                   \"generator_fake_criterion\",\n",
    "        #                                   \"generator_structural_criterion\",\n",
    "        #                                   \"discriminator_criterion\"])\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement forward function.\n",
    "        :param x: Inputs to model.\n",
    "        :return: Outputs of model.\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.generator(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Perform a pass through a batch of training data.\n",
    "        Use Pix2Pix loss function with cross-entropy and L1 structure losses.\n",
    "        See: https://www.tensorflow.org/tutorials/generative/pix2pix#define_the_generator_loss\n",
    "        :param batch: Batch of input, output, weight triplets\n",
    "        :param batch_idx: Index of batch\n",
    "        :return: Loss from this batch of data for use in backprop\n",
    "        \"\"\"\n",
    "        x, y, sample_weight = batch\n",
    "        g_opt, d_opt = self.optimizers()\n",
    "\n",
    "        ####################\n",
    "        # Train Generator\n",
    "        ####################\n",
    "        self.toggle_optimizer(g_opt)\n",
    "\n",
    "        # generate forecasts\n",
    "        fake_forecasts = torch.sigmoid(self.generator(x))\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to fake out discriminator where real==1 and fake==0\n",
    "        g_fake_loss = self.generator_fake_criterion(\n",
    "            d_fake_forecasts, torch.ones_like(d_fake_forecasts)\n",
    "        )\n",
    "        g_fake_loss = torch.mean(\n",
    "            g_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # compute loss to preserve structural similarity of forecasts\n",
    "        g_structural_loss = self.generator_structural_criterion(\n",
    "            fake_forecasts.movedim(-2, 1), y.movedim(-1, 1)\n",
    "        )\n",
    "        g_structural_loss = torch.mean(\n",
    "            g_structural_loss * sample_weight.movedim(-2, 1)\n",
    "        )  # weight spatially\n",
    "\n",
    "        # sum losses with hyperparameter lambda for generator's total loss\n",
    "        g_loss = g_fake_loss + self.generator_lambda * g_structural_loss\n",
    "        self.log(\n",
    "            \"g_train_loss\",\n",
    "            g_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\"g_train_loss_fake\", g_fake_loss, sync_dist=True)\n",
    "        self.log(\"g_train_loss_structural\", g_structural_loss, sync_dist=True)\n",
    "\n",
    "        # manually step generator optimiser\n",
    "        self.manual_backward(g_loss)\n",
    "        g_opt.step()\n",
    "        g_opt.zero_grad()\n",
    "        self.untoggle_optimizer(g_opt)\n",
    "\n",
    "        #####################\n",
    "        # Train Discriminator\n",
    "        #####################\n",
    "        self.toggle_optimizer(d_opt)\n",
    "\n",
    "        # try to detect real forecasts (observations) where real==1 and fake==0\n",
    "        d_real_forecasts = self.discriminator(y, sample_weight)\n",
    "        d_real_loss = self.discriminator_criterion(\n",
    "            d_real_forecasts, torch.ones_like(d_real_forecasts)\n",
    "        )\n",
    "        d_real_loss = (\n",
    "            d_real_loss.mean()\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # generate fake forecasts\n",
    "        fake_forecasts = torch.sigmoid(self.generator(x))\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to detect fake forecasts where real==1 and fake==0\n",
    "        d_fake_loss = self.discriminator_criterion(\n",
    "            d_fake_forecasts, torch.zeros_like(d_fake_forecasts)\n",
    "        )\n",
    "        d_fake_loss = torch.mean(\n",
    "            d_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # sum losses with equal weight\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        self.log(\n",
    "            \"d_train_loss\",\n",
    "            d_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\"d_train_loss_real\", d_real_loss, sync_dist=True)\n",
    "        self.log(\"d_train_loss_fake\", d_fake_loss, sync_dist=True)\n",
    "\n",
    "        # manually step discriminator optimiser\n",
    "        self.manual_backward(d_loss)\n",
    "        d_opt.step()\n",
    "        d_opt.zero_grad()\n",
    "        self.untoggle_optimizer(d_opt)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, sample_weight = batch\n",
    "\n",
    "        ####################\n",
    "        # Validate Generator\n",
    "        ####################\n",
    "\n",
    "        # generate forecasts\n",
    "        fake_forecasts = torch.sigmoid(self.generator(x))\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to fake out discriminator where real==1 and fake==0\n",
    "        g_fake_loss = self.generator_fake_criterion(\n",
    "            d_fake_forecasts, torch.ones_like(d_fake_forecasts)\n",
    "        )\n",
    "        g_fake_loss = torch.mean(\n",
    "            g_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # compute loss to preserve structural similarity of forecasts\n",
    "        g_structural_loss = self.generator_structural_criterion(\n",
    "            fake_forecasts.movedim(-2, 1), y.movedim(-1, 1)\n",
    "        )\n",
    "        g_structural_loss = torch.mean(\n",
    "            g_structural_loss * sample_weight.movedim(-1, 1)\n",
    "        )  # weight spatially\n",
    "\n",
    "        # sum losses with hyperparameter lambda for generator's total loss\n",
    "        g_loss = g_fake_loss + self.generator_lambda * g_structural_loss\n",
    "\n",
    "        # log at epoch-level\n",
    "        self.log(\n",
    "            \"g_val_loss\",\n",
    "            g_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"g_val_loss_fake\",\n",
    "            g_fake_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"g_val_loss_structural\",\n",
    "            g_structural_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        ########################\n",
    "        # Validate Discriminator\n",
    "        ########################\n",
    "\n",
    "        # try to detect real forecasts (observations) where real==1 and fake==0\n",
    "        d_real_forecasts = self.discriminator(y, sample_weight)\n",
    "        d_real_loss = self.discriminator_criterion(\n",
    "            d_real_forecasts, torch.ones_like(d_real_forecasts)\n",
    "        )\n",
    "        d_real_loss = (\n",
    "            d_real_loss.mean()\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to detect fake forecasts where real==1 and fake==0\n",
    "        d_fake_loss = self.discriminator_criterion(\n",
    "            d_fake_forecasts, torch.zeros_like(d_fake_forecasts)\n",
    "        )\n",
    "        d_fake_loss = torch.mean(\n",
    "            d_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # sum losses with equal weight\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        # log at epoch-level\n",
    "        self.log(\n",
    "            \"d_val_loss\",\n",
    "            d_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"d_val_loss_real\",\n",
    "            d_real_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"d_val_loss_fake\",\n",
    "            d_fake_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        ########################\n",
    "        # Forecast Metrics\n",
    "        ########################\n",
    "\n",
    "        # y and y_hat are shape (b, h, w, c, t) but loss expects (b, c, h, w, t)\n",
    "        # note that criterion needs reduction=\"none\" for weighting to work\n",
    "        self.metrics.update(\n",
    "            fake_forecasts.squeeze(dim=-2),\n",
    "            y.squeeze(dim=-1),\n",
    "            sample_weight.squeeze(dim=-1),\n",
    "        )\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(\n",
    "            self.metrics.compute(),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )  # epoch-level metrics\n",
    "        self.metrics.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, sample_weight = batch\n",
    "\n",
    "        ####################\n",
    "        # Test Generator\n",
    "        ####################\n",
    "\n",
    "        # generate forecasts\n",
    "        fake_forecasts = torch.sigmoid(self.generator(x))\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to fake out discriminator where real==1 and fake==0\n",
    "        g_fake_loss = self.generator_fake_criterion(\n",
    "            d_fake_forecasts, torch.ones_like(d_fake_forecasts)\n",
    "        )\n",
    "        g_fake_loss = torch.mean(\n",
    "            g_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # compute loss to preserve structural similarity of binary forecasts\n",
    "        g_structural_loss = self.generator_structural_criterion(\n",
    "            fake_forecasts.movedim(-2, 1), y.movedim(-1, 1)\n",
    "        )\n",
    "        g_structural_loss = torch.mean(\n",
    "            g_structural_loss * sample_weight.movedim(-1, 1)\n",
    "        )  # weight spatially\n",
    "\n",
    "        # sum losses with hyperparameter lambda for generator's total loss\n",
    "        g_loss = g_fake_loss + self.generator_lambda * g_structural_loss\n",
    "\n",
    "        # log and continue\n",
    "        self.log(\n",
    "            \"g_test_loss\", g_loss, on_step=False, on_epoch=True, sync_dist=True\n",
    "        )  # epoch-level loss\n",
    "\n",
    "        ########################\n",
    "        # Test Discriminator\n",
    "        ########################\n",
    "\n",
    "        # try to detect real forecasts (observations) where real==1 and fake==0\n",
    "        d_real_forecasts = self.discriminator(y, sample_weight)\n",
    "        d_real_loss = self.discriminator_criterion(\n",
    "            d_real_forecasts,\n",
    "            torch.ones_like(d_real_forecasts),\n",
    "        )\n",
    "        d_real_loss = (\n",
    "            d_real_loss.mean()\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # pass fake forecasts to discriminator\n",
    "        d_fake_forecasts = self.discriminator(\n",
    "            fake_forecasts, sample_weight.movedim(-2, -1)\n",
    "        )\n",
    "\n",
    "        # try to detect fake forecasts where real==1 and fake==0\n",
    "        d_fake_loss = self.discriminator_criterion(\n",
    "            d_fake_forecasts, torch.zeros_like(d_fake_forecasts)\n",
    "        )\n",
    "        d_fake_loss = torch.mean(\n",
    "            d_fake_loss\n",
    "        )  # weight real/fake loss equally on each instance\n",
    "\n",
    "        # sum losses with equal weight\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        # log and return to optimiser\n",
    "        self.log(\n",
    "            \"d_test_loss\", d_loss, on_step=False, on_epoch=True, sync_dist=True\n",
    "        )  # epoch-level loss\n",
    "\n",
    "        ########################\n",
    "        # Forecast Metrics\n",
    "        ########################\n",
    "\n",
    "        # y and y_hat are shape (b, h, w, c, t) but loss expects (b, c, h, w, t)\n",
    "        # note that criterion needs reduction=\"none\" for weighting to work\n",
    "        self.test_metrics.update(\n",
    "            fake_forecasts.squeeze(dim=-2),\n",
    "            y.squeeze(dim=-1),\n",
    "            sample_weight.squeeze(dim=-1),\n",
    "        )\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict(\n",
    "            self.test_metrics.compute(), on_step=False, on_epoch=True, sync_dist=True\n",
    "        )  # epoch-level metrics\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        \"\"\"\n",
    "        :param batch: Batch of input, output, weight triplets\n",
    "        :param batch_idx: Index of batch\n",
    "        :return: Predictions for given input.\n",
    "        \"\"\"\n",
    "        x, y, sample_weight = batch\n",
    "        y_hat = torch.sigmoid(self.generator(x))\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        g_opt = torch.optim.Adam(self.generator.parameters(), lr=self.learning_rate)\n",
    "        d_opt = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), lr=self.learning_rate * self.d_lr_factor\n",
    "        )\n",
    "        return [g_opt, d_opt], []  # add schedulers to second list if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training UNet model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.focal_loss import sigmoid_focal_loss\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def train_icenet(\n",
    "    configuration_path,\n",
    "    learning_rate,\n",
    "    max_epochs,\n",
    "    batch_size,\n",
    "    n_workers,\n",
    "    filter_size,\n",
    "    n_filters_factor,\n",
    "    seed,\n",
    "    sigma=1,\n",
    "    discriminator_mode=\"forecast\",\n",
    "    generator_fake_criterion=\"ce\",\n",
    "    generator_structural_criterion=\"focal\",\n",
    "    discriminator_criterion=\"ce\",\n",
    "    generator_lambda=500,\n",
    "    d_lr_factor=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train IceNet using the arguments specified in the `args` namespace.\n",
    "    :param args: Namespace of configuration parameters\n",
    "    \"\"\"\n",
    "    # init\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "    # configure datasets and dataloaders\n",
    "    train_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"train\")\n",
    "    val_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"val\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=n_workers,\n",
    "        persistent_workers=True,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=n_workers,\n",
    "        persistent_workers=True,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # construct generator\n",
    "    generator = Generator(\n",
    "        input_channels=train_dataset._num_channels,\n",
    "        filter_size=filter_size,\n",
    "        n_filters_factor=n_filters_factor,\n",
    "        n_forecast_days=train_dataset._n_forecast_days,\n",
    "        sigma=sigma,\n",
    "    )\n",
    "\n",
    "    # construct discriminator\n",
    "    discriminator = Discriminator(\n",
    "        input_channels=train_dataset._num_channels,\n",
    "        filter_size=filter_size,\n",
    "        n_filters_factor=n_filters_factor,\n",
    "        n_forecast_days=train_dataset._n_forecast_days,\n",
    "        mode=discriminator_mode,\n",
    "    )\n",
    "\n",
    "    # configure losses with reduction=\"none\" for sample weighting\n",
    "    if generator_fake_criterion == \"ce\":\n",
    "        generator_fake_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid generator fake loss function: {generator_fake_criterion}.\"\n",
    "        )\n",
    "\n",
    "    if generator_structural_criterion == \"l1\":\n",
    "        generator_structural_criterion = nn.L1Loss(reduction=\"none\")\n",
    "    elif generator_structural_criterion == \"ce\":\n",
    "        generator_structural_criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    elif generator_structural_criterion == \"focal\":\n",
    "        generator_structural_criterion = sigmoid_focal_loss\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid generator structural loss function: {generator_structural_criterion}.\"\n",
    "        )\n",
    "\n",
    "    if discriminator_criterion == \"ce\":\n",
    "        discriminator_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid discriminator loss function: {discriminator_criterion}.\"\n",
    "        )\n",
    "\n",
    "    # configure PyTorch Lightning module\n",
    "    lit_module = LitGAN(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        generator_fake_criterion=generator_fake_criterion,\n",
    "        generator_structural_criterion=generator_structural_criterion,\n",
    "        generator_lambda=generator_lambda,\n",
    "        discriminator_criterion=discriminator_criterion,\n",
    "        learning_rate=learning_rate,\n",
    "        d_lr_factor=d_lr_factor,\n",
    "    )\n",
    "\n",
    "    # set up trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        log_every_n_steps=5,\n",
    "        max_epochs=max_epochs,\n",
    "        num_sanity_val_steps=1,\n",
    "        fast_dev_run=False,  # Runs single batch through train and validation\n",
    "        #    when running trainer.test()\n",
    "        # Note: Cannot use with automatic best checkpointing\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "    trainer.callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # train model\n",
    "    print(\n",
    "        f\"Training {len(train_dataset)} examples / {len(train_dataloader)} batches (batch size {batch_size}).\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validating {len(val_dataset)} examples / {len(val_dataloader)} batches (batch size {batch_size}).\"\n",
    "    )\n",
    "    trainer.fit(lit_module, train_dataloader, val_dataloader)\n",
    "\n",
    "    return lit_module, trainer, checkpoint_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct actual training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 45\n",
    "model, trainer, checkpoint_callback = train_icenet(\n",
    "    configuration_path=dataset_config,\n",
    "    learning_rate=1e-4,\n",
    "    max_epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    n_workers=num_workers,\n",
    "    filter_size=3,\n",
    "    n_filters_factor=1.0,\n",
    "    seed=seed,\n",
    "    sigma=1,\n",
    "    discriminator_mode=\"forecast\",\n",
    "    generator_fake_criterion=\"ce\",\n",
    "    generator_structural_criterion=\"l1\",\n",
    "    discriminator_criterion=\"ce\",\n",
    "    generator_lambda=500,\n",
    "    d_lr_factor=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicts using the best checkpoint from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_k_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = checkpoint_callback.best_model_path\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best result from the checkpoint\n",
    "best_model = LitGAN.load_from_checkpoint(best_checkpoint)\n",
    "\n",
    "# disable randomness, dropout, etc...\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# automatically load the best weights (if best_model isn't added)\n",
    "trainer.test(dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating predictions\")\n",
    "\n",
    "predictions = trainer.predict(best_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker, prediction in enumerate(predictions):\n",
    "    print(f\"Worker: {worker} | Prediction: {prediction.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outputs and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"pytorch_notebook\"\n",
    "network_name = \"api_pytorch_dataset\"\n",
    "output_name = \"example_pytorch_gan_forecast\"\n",
    "output_folder = os.path.join(\n",
    "    \".\", \"results\", \"predict\", output_name, \"{}.{}\".format(network_name, seed)\n",
    ")\n",
    "os.makedirs(output_folder, exist_ok=output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert and output predictions to numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for workers, prediction in enumerate(predictions):\n",
    "    for batch in range(prediction.shape[0]):\n",
    "        date = pd.Timestamp(test_dataset.dates[idx].replace(\"_\", \"-\"))\n",
    "        output_path = os.path.join(output_folder, date.strftime(\"%Y_%m_%d.npy\"))\n",
    "        forecast = prediction[batch, :, :, :, :].movedim(-2, 0)\n",
    "        forecast_np = forecast.detach().cpu().numpy()\n",
    "        np.save(output_path, forecast_np)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a csv file with all the test dates we have predicted for, and to use in generating the final netCDF output using `icenet_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for day in $(seq -w 1 31); do printf \"2019-12-${day}\\n\"; done | tee predict_dates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable.replace('python', 'icenet_output')} -o results/predict example_pytorch_gan_forecast notebook_api_pytorch_data predict_dates.csv -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datetime as dt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "\n",
    "ds = xr.open_dataset(\"results/predict/example_pytorch_gan_forecast.nc\")\n",
    "land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_date = ds.time.values[0]\n",
    "fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "fc[\"time\"] = [\n",
    "    pd.to_datetime(forecast_date) + dt.timedelta(days=int(e)) for e in fc.time.values\n",
    "]\n",
    "\n",
    "# anim = xvid(fc, 15, figsize=4, mask=land_mask)\n",
    "anim = xvid(fc, 15, figsize=(4, 4), mask=land_mask, north=False, south=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check min/max of predicted SIC fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_np[:, :, :, 0].shape)\n",
    "fmin, fmax = np.min(forecast_np[:, :, :, 0]), np.max(forecast_np[:, :, :, 0])\n",
    "print(f\"First forecast day min: {fmin:.4f}, max: {fmax:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original input dataset\n",
    "\n",
    "This is the original input dataset (pre-normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original input dataset (domain not normalised)\n",
    "xr.plot.contourf(\n",
    "    xr.open_dataset(\"data/osisaf/south/siconca/2020.nc\").isel(time=92).ice_conc,\n",
    "    levels=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- IceNet Codebase: v0.2.8"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "seaice_env2 (Conda)",
      "language": "python",
      "name": "sys_seaice_env2"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "seaice_env_min",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
