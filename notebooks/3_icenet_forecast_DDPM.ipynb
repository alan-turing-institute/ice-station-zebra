{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch UNet implementation using IceNet library for data download and post-processing of sea ice forecasting.\n",
    "\n",
    "This notebook has been designed to be independent of other notebooks.\n",
    "\n",
    "### Highlights\n",
    "The key features of this notebook are:\n",
    "* [1. Download](#1.-Download) \n",
    "* [2. Data Processing](#2.-Data-Processing)\n",
    "* [3. Train](#3.-Train)\n",
    "* [4. Prediction](#4.-Prediction)\n",
    "* [5. Outputs and Plotting](#5.-Outputs-and-Plotting)\n",
    "\n",
    "Please note that this notebook relies on a pytorch data loader implementation which is only available from icenet v0.2.8+.\n",
    "\n",
    "To install the necessary python packages, you can use the conda `icenet-notebooks/pytorch/environment.yml` environment file on a Linux system to be able to set-up the necessary pytorch + tensorflow + CUDA + other modules which could be a tricky mix to get working manually:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "### Contributions\n",
    "#### PyTorch implementation of UnetDiffusion\n",
    "Maria Carolina Novitasari\n",
    "\n",
    "#### PyTorch implementation of IceNet\n",
    "\n",
    "Andrew McDonald ([icenet-gan](https://github.com/ampersandmcd/icenet-gan))\n",
    "\n",
    "Bryn Noel Ubald (Refactor, updates for daily predictions and matching icenet library)\n",
    "\n",
    "#### Notebook\n",
    "Bryn Noel Ubald (author)\n",
    "\n",
    "#### PyTorch Integration\n",
    "Bryn Noel Ubald\n",
    "\n",
    "Ryan Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Download Daily Data for IceNet\n",
    "\n",
    "#### DOWNLOAD SIC Data  \n",
    "\n",
    "To download Sea Ice Concentration (SIC) data, modify the script below with the desired date range:\n",
    "\n",
    "```python\n",
    "sic = SICDownloader(\n",
    "    dates=[\n",
    "        pd.to_datetime(date).date()  # Dates to download SIC data for\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-12-31\", freq=\"D\")\n",
    "    ],\n",
    "    delete_tempfiles=True,           # Delete temporary downloaded files after use\n",
    "    north=False,                     # Use mask for the Northern Hemisphere (set to True if needed)\n",
    "    south=True,                      # Use mask for the Southern Hemisphere\n",
    "    parallel_opens=True,             # Enable parallel processing with dask.delayed\n",
    ")\n",
    "\n",
    "sic.download()\n",
    "```\n",
    "\n",
    "#### Download ERA5 Data  \n",
    "\n",
    "##### Setup ERA5 API\n",
    "\n",
    "Use the following link to set up the ERA5 API: [https://cds.climate.copernicus.eu/how-to-api?](https://cds.climate.copernicus.eu/how-to-api?).\n",
    "\n",
    "Run the following script with your desired dates:\n",
    "\n",
    "#### ERA5 Downloader  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "\n",
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],      # Name of variables to download\n",
    "    dates=[                                     # Dates to download the variable data for\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-12-31\", freq=\"D\")\n",
    "    ],\n",
    "    path=\"./data\",                              # Location to download data to (default is `./data`)\n",
    "    delete_tempfiles=True,                      # Whether to delete temporary downloaded files\n",
    "    levels=[None, [250, 500], None, None],      # The levels at which to obtain the variables for (e.g. for zg, it is the pressure levels)\n",
    "    max_threads=4,                              # Maximum number of concurrent downloads\n",
    "    north=False,                                # Boolean: Whether require data across northern hemisphere\n",
    "    south=True,                                 # Boolean: Whether require data across southern hemisphere\n",
    "    use_toolbox=False)                          # Experimental, alternative download method\n",
    "\n",
    "era5.download()                                 # Start downloading\n",
    "```\n",
    "\n",
    "The prototype data currently in use (South Pole, 2020) can be downloaded from **Baskerville** at the following path: `/vjgo8416-ice-frcst/shared/prototype_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple\n",
    "from torchmetrics import Metric\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from torchmetrics import MetricCollection\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "print(os.getcwd())\n",
    "\n",
    "from models import GaussianDiffusion, UNetDiffusion, LitDiffusion\n",
    "from trainers import train_diffusion_icenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "sys.stdout = open(f'training_logs/training_log_{timestamp}.txt', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unset SLURM_NTASKS if it's causing issues\n",
    "if \"SLURM_NTASKS\" in os.environ:\n",
    "    del os.environ[\"SLURM_NTASKS\"]\n",
    "\n",
    "# Optionally, set SLURM_NTASKS_PER_NODE if needed\n",
    "os.environ[\"SLURM_NTASKS_PER_NODE\"] = \"1\"  # or whatever value is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data\n",
    "\n",
    "Create masks for masking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate and Sea Ice data\n",
    "\n",
    "Download climate variables from ERA5 and sea ice concentration from OSI-SAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],\n",
    "    levels=[None, [250, 500], None, None],\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    max_threads=64,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    # NOTE: there appears to be a bug with the toolbox API at present (icenet#54)\n",
    "    use_toolbox=False\n",
    ")\n",
    "\n",
    "# era5.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic = SICDownloader(\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    parallel_opens=False,\n",
    ")\n",
    "\n",
    "# sic.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-grid ERA5 reanalysis data, and rotate wind vector data from ERA5 to align with EASE2 projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5.regrid()\n",
    "era5.rotate_wind_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Process downloaded datasets.\n",
    "\n",
    "To make life easier, setting up train, val, test dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-03-31\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-03\", \"2020-04-23\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    ")\n",
    "processed_name = \"notebook_api_pytorch_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor\n",
    "\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],\n",
    "    [\"tas\", \"zg500\", \"zg250\"],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "\n",
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],\n",
    "    [],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causes hanging on training, when generating sample.\n",
    "pp.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "pp.process()\n",
    "\n",
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "As with the `icenet_dataset_create` command we can create a dataset configuration for training the network. As before this can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_pytorch_data.json\"\n",
    "dataset_name = \"notebook_api_pytorch_data\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=1,\n",
    "    generate_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.notebook_api_pytorch_data.json` (recall we set the dataset name as `notebook_api_pytorch_data` above).\n",
    "\n",
    "In this case, for pytorch, will read data in directly, rather than using cached tfrecords inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.write_dataset_config_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the IceNetDataSet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSetPyTorch\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "We implement a custom PyTorch class for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IceNet2 U-Net Diffusion model\n",
    "\n",
    "Maria's work (PyTorch Diffusion using U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super().__init__()\n",
    "        self.interp = F.interpolate\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom metrics for use in validation and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _LightningModule_ wrapper for UNetDiffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training UNetDiffusion model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct actual training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 45\n",
    "\n",
    "# Training configuration\n",
    "learning_rate = 1e-5 #1e-4\n",
    "max_epochs = 1 #500\n",
    "filter_size = 3\n",
    "n_filters_factor = 0.5\n",
    "timesteps = 1000\n",
    "\n",
    "batch_size = 8 #8 #4 #16 #32 #64 #16\n",
    "shuffle = False\n",
    "persistent_workers=True\n",
    "num_workers = 8\n",
    "\n",
    "print(\"batch_size...\", batch_size)\n",
    "print(\"num_workers...\", num_workers)\n",
    "\n",
    "# Print all training parameters\n",
    "print(\"Training parameters:\")\n",
    "print(f\"  configuration_path: {dataset_config}\")\n",
    "print(f\"  learning_rate: {learning_rate}\")\n",
    "print(f\"  max_epochs: {max_epochs}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  n_workers: {num_workers}\")\n",
    "print(f\"  filter_size: {filter_size}\")\n",
    "print(f\"  n_filters_factor: {n_filters_factor}\")\n",
    "print(f\"  seed: {seed}\")\n",
    "print(f\"  timesteps: {timesteps}\")\n",
    "\n",
    "# Call the training function\n",
    "model, trainer, checkpoint_callback = train_diffusion_icenet(\n",
    "    configuration_path=dataset_config,\n",
    "    learning_rate=learning_rate,\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    n_workers=num_workers,\n",
    "    filter_size=filter_size,\n",
    "    n_filters_factor=n_filters_factor,\n",
    "    seed=seed,\n",
    "    timesteps=timesteps,\n",
    "    persistent_workers=persistent_workers\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicts using the best checkpoint from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_k_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = checkpoint_callback.best_model_path\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best result from the checkpoint\n",
    "# best_model = LitUNet.load_from_checkpoint(best_checkpoint)\n",
    "\n",
    "#mc\n",
    "best_model = LitDiffusion.load_from_checkpoint(best_checkpoint)\n",
    "\n",
    "# disable randomness, dropout, etc...\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                             persistent_workers=persistent_workers, shuffle=False)\n",
    "\n",
    "# automatically load the best weights (if best_model isn't added)\n",
    "trainer.test(dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cosine results\n",
    "# [{'test_loss': 0.48640450835227966,\n",
    "#   'test_accuracy': 51.62199783325195,\n",
    "#   'test_accuracy_0': 51.56485366821289,\n",
    "#   'test_accuracy_1': 51.77238464355469,\n",
    "#   'test_accuracy_2': 52.235984802246094,\n",
    "#   'test_accuracy_3': 51.62677764892578,\n",
    "#   'test_accuracy_4': 51.081172943115234,\n",
    "#   'test_accuracy_5': 51.31882858276367,\n",
    "#   'test_accuracy_6': 51.75397872924805,\n",
    "#   'test_sieerror': 729145600.0,\n",
    "#   'test_sieerror_0': 104656872.0,\n",
    "#   'test_sieerror_1': 104813752.0,\n",
    "#   'test_sieerror_2': 104287504.0,\n",
    "#   'test_sieerror_3': 104360624.0,\n",
    "#   'test_sieerror_4': 104103128.0,\n",
    "#   'test_sieerror_5': 103494376.0,\n",
    "#   'test_sieerror_6': 103429376.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating predictions\")\n",
    "\n",
    "predictions = trainer.predict(best_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker, prediction in enumerate(predictions):\n",
    "    print(f\"Worker: {worker} | Prediction: {prediction.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outputs and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"pytorch_notebook\"\n",
    "network_name = \"api_pytorch_dataset\"\n",
    "output_name = \"example_pytorch_forecast_diff\"\n",
    "output_folder = os.path.join(\".\", \"results\", \"predict\", output_name,\n",
    "                                \"{}.{}\".format(network_name, seed))\n",
    "os.makedirs(output_folder, exist_ok=output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert and output predictions to numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for workers, prediction in enumerate(predictions):\n",
    "    for batch in range(prediction.shape[0]):\n",
    "        date = pd.Timestamp(test_dataset.dates[idx].replace('_', '-'))\n",
    "        output_path = os.path.join(output_folder, date.strftime(\"%Y_%m_%d.npy\"))\n",
    "        print(\"prediction shape...\",prediction.shape)\n",
    "        # forecast = prediction[batch, :, :, :, :].movedim(-2, 0)\n",
    "        forecast = prediction[batch, :, :, :].movedim(-1, 0)\n",
    "        forecast_np = forecast.detach().cpu().numpy()\n",
    "        np.save(output_path, forecast_np)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a csv file with all the test dates we have predicted for, and to use in generating the final netCDF output using `icenet_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !printf \"2020-04-01\\n2020-04-02\" | tee testdates_diff.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !icenet_output -m -o results/predict example_pytorch_forecast_diff notebook_api_pytorch_data testdates_diff.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Change this to the actual version dir\n",
    "log_dir = \"lightning_logs/version_1065922\" #_batch8_sxx8\"\n",
    "# version_1064080\" #version_1063301\"\n",
    "# log_dir = f\"lightning_logs/version_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Load the logs\n",
    "event_acc = EventAccumulator(log_dir)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List all scalar tags to find the correct name\n",
    "print(\"Available tags:\", event_acc.Tags()['scalars'])\n",
    "\n",
    "# Get the scalar events for val_loss\n",
    "val_loss_events = event_acc.Scalars('val_loss')\n",
    "\n",
    "# FIX: Use index as epoch number instead of .step\n",
    "steps = list(range(1, len(val_loss_events) + 1))\n",
    "values = [e.value for e in val_loss_events]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, values, label='Validation Loss', color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your actual path\n",
    "log_dir = \"lightning_logs/version_1065922\" #_stop_needto_finetune\"\n",
    "# log_dir = f\"lightning_logs/version_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "event_acc = EventAccumulator(log_dir)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List all available scalar tags\n",
    "print(\"Available scalar tags:\")\n",
    "print(event_acc.Tags()['scalars'])  # e.g. val_accuracy, val_accuracy_0, etc.\n",
    "\n",
    "# Get accuracy for all lead times (overall accuracy)\n",
    "accuracy_events = event_acc.Scalars('val_accuracy')\n",
    "\n",
    "# FIX: Use index as epoch number instead of .step\n",
    "steps = list(range(1, len(accuracy_events) + 1))\n",
    "values = [e.value for e in accuracy_events]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, values, label='Validation Accuracy', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datetime as dt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from icenet.plotting.video import xarray_to_video as xvid\n",
    "# from icenet.data.sic.mask import Masks\n",
    "\n",
    "# ds = xr.open_dataset(\"results/predict/example_pytorch_forecast_diff.nc\")\n",
    "# land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "# ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_date = ds.time.values[0]\n",
    "# fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "# fc['time'] = [pd.to_datetime(forecast_date) \\\n",
    "#               + dt.timedelta(days=int(e)) for e in fc.time.values]\n",
    "\n",
    "# anim = xvid(fc, 15, figsize=(4,4), mask=land_mask)\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check min/max of predicted SIC fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( forecast_np[:, :, :, 0].shape )\n",
    "# fmin, fmax = np.min(forecast_np[:, :, :, 0]), np.max(forecast_np[:, :, :, 0])\n",
    "# print( f\"First forecast day min: {fmin:.4f}, max: {fmax:.4f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original input dataset\n",
    "\n",
    "This is the original input dataset (pre-normalisation) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load original input dataset (domain not normalised)\n",
    "# xr.plot.contourf(xr.open_dataset(\"data/osisaf/south/siconca/2020.nc\").isel(time=92).ice_conc, levels=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- IceNet Codebase: v0.2.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seaice_env2 (Conda)",
   "language": "python",
   "name": "sys_seaice_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
