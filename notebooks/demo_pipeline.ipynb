{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de07aa6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Ice Station Zebra Pipeline Demo\n",
    "\n",
    "This demonstration showcases the complete Ice Station Zebra ML pipeline capabilities through CLI commands. \n",
    "\n",
    "**Target Audience:** Developer teams and future team members who want to understand our design decisions, \n",
    "trade-offs, and flexible experimentation capabilities.\n",
    "\n",
    "**You'll learn how to:**\n",
    "- Run our training pipeline end-to-end in three lines of code\n",
    "- Swap between different modelling paradigms\n",
    "- Reproduce runs and inspect the outputs\n",
    "- Evaluate the performance of the models in line with community standards on sea ice forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcab5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*repr.*Field.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46684f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "If you want to run this notebook, you will need a **CDS account** in order to download the ERA5 weather data. Details of how to set this up can be found [here](https://cds.climate.copernicus.eu/how-to-api).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac158b",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "[**Section 1: End-to-End Training**](#section-1-end-to-end-training-pipeline)\n",
    "- Run a full zebra pipeline end-to-end using a minimal configuration & data\n",
    "- Inspect training artifacts and see evaluation outputs\n",
    "\n",
    "[**Section 2: Model Flexibility**](#section-2-model-flexibility)\n",
    "- Switch between Encode-Process-Decode paradigm and standalone persistence model\n",
    "- Explore Encoder module functionality (Multimodality)\n",
    "\n",
    "[**Section 3: Full flexibility - Advanced Example**](#section-3-full-flexibility---advanced-example)\n",
    "- Write or adapt config files to change pipeline behavior\n",
    "- use anemoi functionality to fetch and inspect standard datsets\n",
    "- see our pipeline data checks and validation in action\n",
    "- Evaluate and compare model performance using a pretrained model checkpoint\n",
    "- Explore different plotting formats and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd4099",
   "metadata": {},
   "source": [
    "# Section 1: End-to-End Training Pipeline\n",
    "\n",
    "In this section, we'll demonstrate the complete training pipeline using a simple **UNet model with a naive encoder / decoder** (more details of this can be found below in [section 2](#section-2-model-flexibility)). \n",
    "The dataset contains sea ice concentration data (OSISAF) and corresponding atmospheric data (ERA5).\n",
    "We don't expect the model to do well as we are only training for 10 epochs, and won't do any hyperparameter optimisation. However, it will give us a sense of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bb24c",
   "metadata": {},
   "source": [
    "You can install the repo by running the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/alan-turing-institute/ice-station-zebra\n",
    "cd ice-station-zebra\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a006a",
   "metadata": {},
   "source": [
    "### Environment Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08624a",
   "metadata": {},
   "source": [
    "Let's verify that our zebra cli tools are available and working.\n",
    "\n",
    "To run this notebook, you'll need a kernel (e.g. conda or .venv) with the ice_station_zebra repo and jupyter installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b83ac5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mzebra [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                                      \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " Entrypoint for zebra application commands                                      \n",
      "                                                                                \n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Options \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m            Install completion for the current shell.    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m               Show completion for the current shell, to    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                 copy it or customize the installation.       \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                \u001b[1;32m-h\u001b[0m        Show this message and exit.                  \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mdatasets \u001b[0m\u001b[1;36m \u001b[0m Manage datasets                                                   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mevaluate \u001b[0m\u001b[1;36m \u001b[0m Evaluate a model.                                                 \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mtrain    \u001b[0m\u001b[1;36m \u001b[0m Train a model.                                                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!zebra --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f835b9c",
   "metadata": {},
   "source": [
    "### Download the dataset for running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb2db9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"> \n",
    "\n",
    "N.B. to run this minimal example requires you to have signed up to access the ERA5 data. Details on how to do so can be found [here](https://cds.climate.copernicus.eu/how-to-api).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f75076",
   "metadata": {},
   "source": [
    "The model will download a set of ERA5 weather data, and OSISAF sea ice concentration (SIC) data over 2017-2019. The details of this data are specified in `demo_nb.yaml`. (Details about how these config files work will be covered in a later section.) \n",
    "\n",
    "If the data is already present, a summary of the dataset will be printed (it will not be downloaded again). This same summary can be created by running `zebra datasets inspect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a9908",
   "metadata": {},
   "source": [
    "**This assumes you have a folder called `my_data/` in the root of the repo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e537097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on samp-sicsouth-osisaf-25k-2017-2019-24h-v1.\n",
      "Inspecting dataset samp-sicsouth-osisaf-25k-2017-2019-24h-v1 at /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-sicsouth-osisaf-25k-2017-2019-24h-v1.zarr.\n",
      "ğŸ“¦ Path          : /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-sicsouth-osisaf-25k-2017-2019-24h-v1.zarr\n",
      "ğŸ”¢ Format version: 0.30.0\n",
      "\n",
      "ğŸ“… Start      : 2017-01-01 00:00\n",
      "ğŸ“… End        : 2019-01-31 00:00\n",
      "â° Frequency  : 1d\n",
      "ğŸš« Missing    : 0\n",
      "ğŸŒ Resolution : None\n",
      "ğŸŒ Field shape: [432, 432]\n",
      "\n",
      "ğŸ“ Shape      : 761 Ã— 1 Ã— 1 Ã— 186,624 (541.8 MiB)\n",
      "ğŸ’½ Size       : 51.5 MiB (51.5 MiB)\n",
      "ğŸ“ Files      : 804\n",
      "\n",
      "   Index â”‚ Variable â”‚ Min â”‚ Max â”‚      Mean â”‚    Stdev\n",
      "   â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       0 â”‚ ice_conc â”‚   0 â”‚   1 â”‚ 0.0715942 â”‚ 0.237269\n",
      "   â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  2025-11-05 17:04:19.870744 : initialised\n",
      "  2025-11-05 17:04:19.874376 : tmp_statistics_initialised (version=3)\n",
      "  2025-11-05 17:04:19.894319 : init finished\n",
      "  2025-11-05 17:04:33.822868 : compute_statistics_end\n",
      "\n",
      "ğŸ”‹ Dataset ready, last update 4 hours ago.\n",
      "ğŸ“Š Statistics ready.\n",
      "\n",
      "Dataset samp-sicsouth-osisaf-25k-2017-2019-24h-v1 already exists at /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-sicsouth-osisaf-25k-2017-2019-24h-v1.zarr, no need to download.\n",
      "Working on samp-weathersouth-era5-0p5-2017-2019-24h-v1.\n",
      "Inspecting dataset samp-weathersouth-era5-0p5-2017-2019-24h-v1 at /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-weathersouth-era5-0p5-2017-2019-24h-v1.zarr.\n",
      "ğŸ“¦ Path          : /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-weathersouth-era5-0p5-2017-2019-24h-v1.zarr\n",
      "ğŸ”¢ Format version: 0.30.0\n",
      "\n",
      "ğŸ“… Start      : 2017-01-01 00:00\n",
      "ğŸ“… End        : 2019-01-31 00:00\n",
      "â° Frequency  : 1d\n",
      "ğŸš« Missing    : 0\n",
      "ğŸŒ Resolution : 0.5\n",
      "ğŸŒ Field shape: [181, 720]\n",
      "\n",
      "ğŸ“ Shape      : 761 Ã— 27 Ã— 1 Ã— 130,320 (10 GiB)\n",
      "ğŸ’½ Size       : 5.1 GiB (5.1 GiB)\n",
      "ğŸ“ Files      : 804\n",
      "\n",
      "   Index â”‚ Variable       â”‚          Min â”‚         Max â”‚        Mean â”‚       Stdev\n",
      "   â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       0 â”‚ 10u            â”‚     -34.5022 â”‚     29.7513 â”‚    0.211512 â”‚     6.37366\n",
      "       1 â”‚ 10v            â”‚     -32.9062 â”‚     34.5098 â”‚    0.625931 â”‚     5.28895\n",
      "       2 â”‚ 2t             â”‚      194.151 â”‚     316.578 â”‚     276.006 â”‚     23.8954\n",
      "       3 â”‚ cos_julian_day â”‚    -0.999979 â”‚           1 â”‚  -0.0815566 â”‚    0.717394\n",
      "       4 â”‚ msl            â”‚      92090.6 â”‚      106753 â”‚      100563 â”‚     1517.43\n",
      "       5 â”‚ q_10           â”‚  1.99714e-06 â”‚ 3.98833e-06 â”‚  3.1848e-06 â”‚ 3.33937e-07\n",
      "       6 â”‚ q_1000         â”‚  1.00984e-06 â”‚   0.0241617 â”‚  0.00685176 â”‚  0.00582725\n",
      "       7 â”‚ q_250          â”‚ -2.78626e-05 â”‚  0.00097467 â”‚ 5.37709e-05 â”‚ 7.26709e-05\n",
      "       8 â”‚ q_500          â”‚ -3.64147e-05 â”‚  0.00847122 â”‚ 0.000775763 â”‚  0.00106908\n",
      "       9 â”‚ sin_julian_day â”‚    -0.999999 â”‚    0.999986 â”‚    0.144836 â”‚    0.676548\n",
      "      10 â”‚ sp             â”‚      54185.4 â”‚      104742 â”‚     94823.6 â”‚     11649.6\n",
      "      11 â”‚ t_10           â”‚      176.521 â”‚     269.056 â”‚     224.874 â”‚     14.6091\n",
      "      12 â”‚ t_1000         â”‚      218.124 â”‚     319.434 â”‚     279.768 â”‚     17.0561\n",
      "      13 â”‚ t_250          â”‚      193.591 â”‚     245.752 â”‚     222.081 â”‚     9.34345\n",
      "      14 â”‚ t_500          â”‚      217.287 â”‚     278.775 â”‚     252.209 â”‚     13.6165\n",
      "      15 â”‚ u_10           â”‚     -75.5068 â”‚     148.576 â”‚     8.38132 â”‚     29.8197\n",
      "      16 â”‚ u_1000         â”‚     -34.0149 â”‚      28.547 â”‚    0.278045 â”‚      6.8532\n",
      "      17 â”‚ u_250          â”‚     -58.6453 â”‚     111.279 â”‚     14.7633 â”‚     18.3579\n",
      "      18 â”‚ u_500          â”‚     -46.3319 â”‚     76.1621 â”‚     7.61019 â”‚     12.6977\n",
      "      19 â”‚ v_10           â”‚     -100.695 â”‚     107.326 â”‚   0.0888905 â”‚     9.39261\n",
      "      20 â”‚ v_1000         â”‚     -31.5624 â”‚     34.0895 â”‚    0.625456 â”‚     5.69493\n",
      "      21 â”‚ v_250          â”‚     -89.0778 â”‚     95.2256 â”‚   -0.130668 â”‚     13.6715\n",
      "      22 â”‚ v_500          â”‚     -69.3972 â”‚     64.2349 â”‚  -0.0456233 â”‚      9.4469\n",
      "      23 â”‚ z_10           â”‚       261776 â”‚      313778 â”‚      298404 â”‚     11996.4\n",
      "      24 â”‚ z_1000         â”‚     -6336.16 â”‚     3742.49 â”‚     396.374 â”‚     1227.31\n",
      "      25 â”‚ z_250          â”‚      86824.3 â”‚      108772 â”‚      100552 â”‚     6056.51\n",
      "      26 â”‚ z_500          â”‚      43615.6 â”‚     58569.9 â”‚     53578.1 â”‚     3748.14\n",
      "   â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  2025-11-05 17:05:15.565398 : initialised\n",
      "  2025-11-05 17:05:15.569808 : tmp_statistics_initialised (version=3)\n",
      "  2025-11-05 17:05:15.591410 : init finished\n",
      "  2025-11-05 18:43:08.631198 : compute_statistics_end\n",
      "\n",
      "ğŸ”‹ Dataset ready, last update 2 hours ago.\n",
      "ğŸ“Š Statistics ready.\n",
      "\n",
      "Dataset samp-weathersouth-era5-0p5-2017-2019-24h-v1 already exists at /Users/sarana/Documents/e&s/SeaIce/ice-station-zebra/my_data/data/anemoi/samp-weathersouth-era5-0p5-2017-2019-24h-v1.zarr, no need to download.\n"
     ]
    }
   ],
   "source": [
    "!zebra datasets create --config-name=demo_nb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad9b6a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c115c5",
   "metadata": {},
   "source": [
    "The next command will train a simple UNet model for sea ice concentration forecasting, and the training will run for 10 epochs. The options used for training this model are specified in `demo_nb.yaml`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b60a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_MODE=offline !zebra train --config-name=demo_nb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb543d31",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b7404",
   "metadata": {},
   "source": [
    "Finally the model is evaluated using the checkpoint, saved during the training run. The results are then logged to Weights & Biases. (There is a wandb account for the Sea Ice project where results are logged by default.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0263c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/bin/zebra\", line 3, in <module>\n",
      "    from ice_station_zebra.cli import app\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/cli/__init__.py\", line 2, in <module>\n",
      "    from .main import app\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/cli/main.py\", line 4, in <module>\n",
      "    from ice_station_zebra.data_processors import datasets_cli\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/data_processors/__init__.py\", line 1, in <module>\n",
      "    from .cli import datasets_cli\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/data_processors/cli.py\", line 8, in <module>\n",
      "    from .filters import register_filters\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/data_processors/filters/__init__.py\", line 3, in <module>\n",
      "    from .doubling_filter import DoublingFilter\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/ice_station_zebra/data_processors/filters/doubling_filter.py\", line 4, in <module>\n",
      "    from anemoi.transform.filters.matching import MatchingFieldsFilter, matching\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/anemoi/transform/filters/matching.py\", line 22, in <module>\n",
      "    import numpy as np\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/numpy/__init__.py\", line 130, in <module>\n",
      "    from numpy.__config__ import show as show_config\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/numpy/__config__.py\", line 4, in <module>\n",
      "    from numpy.core._multiarray_umath import (\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/numpy/core/__init__.py\", line 72, in <module>\n",
      "    from . import numerictypes as nt\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/numpy/core/numerictypes.py\", line 105, in <module>\n",
      "    from ._type_aliases import (\n",
      "  File \"/opt/miniconda3/envs/ice_station_zebra/lib/python3.12/site-packages/numpy/core/_type_aliases.py\", line 23, in <module>\n",
      "    from numpy.core._dtype import _kind_name\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1091, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1191, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!zebra evaluate --config-name=demo_nb.yaml --checkpoint=\"../my_data/training/naive_unet_naive_demo_south/wandb/latest-run/checkpoints/epoch=9-step=1810.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c7a36",
   "metadata": {},
   "source": [
    "We can then check the predictions from this model. (N.b. as it is only run for a small number of epochs on a limited dataset, the results are not great.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce20c54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../my_data/training/naive_unet_naive_demo_south/wandb/latest-run/files/media/videos/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# extract the file name (it has a random string on the end)\u001b[39;00m\n\u001b[32m      5\u001b[39m folder = \u001b[33m\"\u001b[39m\u001b[33m../my_data/training/naive_unet_naive_demo_south/wandb/latest-run/files/media/videos/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m file = os.path.join(folder, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m      8\u001b[39m Image(filename=file)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../my_data/training/naive_unet_naive_demo_south/wandb/latest-run/files/media/videos/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "# extract the file name (it has a random string on the end)\n",
    "folder = \"../my_data/training/naive_unet_naive_demo_south/wandb/latest-run/files/media/videos/\"\n",
    "file = os.path.join(folder, os.listdir(folder)[0])\n",
    "\n",
    "Image(filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c063cb",
   "metadata": {},
   "source": [
    "# Section 2: Model Flexibility\n",
    "\n",
    "In this section, we'll demonstrate how easy it is to switch between different model architectures.\n",
    "We'll also show the difference between standalone models and processor models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3896d",
   "metadata": {},
   "source": [
    "The conceptually simpler model type is a standalone model, which takes in the input data and directly outputs the prediction. These models are less flexible, as they have to be specifically coded to handle new input / output data. Consequently a separate instance of the model is likely to be needed for each input / output combination. However, the input variables are available without transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86e449",
   "metadata": {},
   "source": [
    "![pipeline_standalone](../docs/assets/pipeline-standalone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2aba6",
   "metadata": {},
   "source": [
    "The more complex model type is processor model, which uses an encode-process-decode paradigm. Here, the input data is first encoded into a latent representation, which is then processed by a core model, before being decoded back into the output space. This allows for more flexibility in terms of input and output variables, as well as the ability to use different types of models for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ee90b",
   "metadata": {},
   "source": [
    "![pipeline_encode_process_decode](../docs/assets/pipeline-encode-process-decode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e023e30",
   "metadata": {},
   "source": [
    "## Using an alternative processor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8a81e",
   "metadata": {},
   "source": [
    "The initial model we ran, the `naive_unet_naive` model, is an example of a processor model. It uses a naive encoder, to convert the parameters into the right dimensions for the latent space. A UNet model is then run on the latent space, before a naive encoder extracts the SIC predictions. \n",
    "\n",
    "In this example, we switch the `naive_unet_naive` model for a more complex `naive_vit_naive` model, which still uses a naive enocder / decoder, but replaces the UNet model with a Vision Transformer (ViT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456831c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zebra train --config-name=demo_nb.yaml model=naive_vit_naive loggers.wandb.name=naive_vit_naive_demo_south\n",
    "# the loggers.wandb.name is for convenience of plotting, and is not a required argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zebra evaluate --config-name=demo_nb.yaml --checkpoint=\"../my_data/training/naive_vit_naive_demo_south/wandb/latest-run/checkpoints/epoch=9-step=1810.ckpt\" loggers.wandb.name=naive_vit_naive_demo_south\n",
    "# the loggers.wandb.name is for convenience of plotting, and is not a required argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e3a9e",
   "metadata": {},
   "source": [
    "The results are logged to [Weights & Biases](https://wandb.ai/), but they can also be viewed locally. When inspecting the predictions of the model using a vision transformer architecture you can notice a checkerboard pattern, an artefact of the patch embedding approach this model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the file name (it has a random string on the end)\n",
    "folder = \"../my_data/training/naive_vit_naive_demo_south/wandb/latest-run/files/media/videos/\"\n",
    "file = os.path.join(folder, os.listdir(folder)[0])\n",
    "\n",
    "Image(filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d91e3",
   "metadata": {},
   "source": [
    "\n",
    "There is also the option to replace the naive encoder / decoder with convolutional neural networks (CNNs), however these struggle to train on a laptop, so we won't demonstrate them here. If you want to test them out, just set the model to be `cnn_unet_cnn` or `cnn_vit_cnn` in the config file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f381557",
   "metadata": {},
   "source": [
    "## Standalone persistence model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1dcb4",
   "metadata": {},
   "source": [
    "As mentioned above, an alternative form of model doesn't use an encoder / decoder architecture. We can demonstrate this with a `persistence model` which simply outputs the last input frame as the prediction.\n",
    "\n",
    "Note, that the easiest way to run the `persistence` model is to use the `persistence.yaml` config file. As this config file is not set-up to be run in this notebook (unlike the `demo_nb.yaml` file we've been using so far), we need to specify a few extra options in the command line to ensure it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zebra train --config-name=persistence.yaml ++base_path=\"../my_data\" loggers.wandb.save_dir=\"../my_data/training/persistence_demo_south\"\n",
    "# note the base_path and the loggers.wandb.save_dir command were not needed in the previous examples as they were specified in the demo_nb.yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e43bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zebra evaluate --config-name=persistence.yaml ++base_path=\"../my_data\" --checkpoint=\"../my_data/training/persistence_demo_south/wandb/latest-run/checkpoints/epoch=1-step=0.ckpt\" loggers.wandb.save_dir=\"../my_data/training/persistence_demo_south\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the file name (it has a random string on the end)\n",
    "folder = \"../my_data/training/persistence_demo_south/wandb/latest-run/files/media/videos/\"\n",
    "file = os.path.join(folder, os.listdir(folder)[0])\n",
    "\n",
    "Image(filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb89f6",
   "metadata": {},
   "source": [
    "# Section 3: Full flexibility - Advanced Example\n",
    "\n",
    "This section shows how to adapt the different parts of the model pipeline to your needs. We will look in to the config files that are the basis of the pipeline, and we will show how to create your own config file to give you full control over the model training. \n",
    "\n",
    "The pipeline uses hydra, which is a powerful configuration management tool. More details can be found [here](https://hydra.cc/docs/intro/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ade780",
   "metadata": {},
   "source": [
    "## The base config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640639a9",
   "metadata": {},
   "source": [
    "So far in this notebook, we've mostly used the `demo_nb.yaml` config file. We can have a look at the contents of this file to see what options it is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/demo_nb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef716c",
   "metadata": {},
   "source": [
    "As you can see here, the defaults section points to the `base.yaml` config file, which contains the main options for the pipeline. There are then some specific options for how many epochs to train for, and where to save the data.\n",
    "\n",
    "So lets have a look at the `base.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/base.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962263b1",
   "metadata": {},
   "source": [
    "There are lots more options under the defaults section here, for specifying a range of options such as the datasets, or the model to use. Some of these options should also look familiar from the `demo_nb.yaml` file. For example, under `loggers`, we can see `wandb` specified as the logger to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360b12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py          \u001b[34mevaluate\u001b[m\u001b[m             \u001b[34mpredict\u001b[m\u001b[m\n",
      "base.yaml            full.yaml            \u001b[34msplit\u001b[m\u001b[m\n",
      "\u001b[34mdatasets\u001b[m\u001b[m             \u001b[34mloggers\u001b[m\u001b[m              \u001b[34mtrain\u001b[m\u001b[m\n",
      "demo_nb.yaml         \u001b[34mmodel\u001b[m\u001b[m                whale_corridors.yaml\n",
      "demo_north.yaml      \u001b[34mnotebooks\u001b[m\u001b[m\n",
      "demo.yaml            persistence.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls ../ice_station_zebra/config/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f62c3",
   "metadata": {},
   "source": [
    "Looking at the contents of the `ice_station_zebra/config/` folder, you can see that all the defaults in the base config, map to specific folders / files. Each file contains a different set of options that are being used for different parts of the pipeline. You can also see the alternative files that could be substituted in (e.g. using `naive_vit_naive.yaml` instead of `naive_unet_naive.yaml` for the model).\n",
    "\n",
    "If you only want to change a single parameter, rather than the whole config file, you can specify this in the config, referencing the nested structure. An example of this is how we specified the number of epochs to train for in the `demo_nb.yaml` file. Here, we overrode the default of 50 epochs specified in `train/trainer/default.yaml`, by setting `max_epochs` to be 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c093392",
   "metadata": {},
   "source": [
    "You might remember, we also used the `persistence.yaml` config file to run a persistence model. We can look at the contents of that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/persistence.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60553fd",
   "metadata": {},
   "source": [
    "Here you can see that it mostly uses the same parameters as the `base` config. However, it overrides the default configs for `train` and `model` to switch in the persistence model options. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d25c11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Typically you should not change the values in the config files. If you want to change some of these options, the best way to do so is to create your own config file, similar to `demo_nb.yaml`, It's easiest to use the `base` config as the default, and then specify the parameters you want to change using `override` or by specifying the parameter in the config hierarchy. \n",
    "\n",
    "When you want to run code using that config file, you can specify it using the `--config-name` option in the CLI command. This will remove the need to specify the individual parameters in the command line each time. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a07f5",
   "metadata": {},
   "source": [
    "## Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044301a",
   "metadata": {},
   "source": [
    "The first step in the model pipeline is to download the required input data. This has been developed to make use of the [anemoi datasets](https://anemoi.readthedocs.io/projects/datasets/en/latest/) package, which is part of the [anemoi toolkit](https://anemoi.readthedocs.io/en/latest/) developed by ECMWF. \n",
    "\n",
    "New datasets can be downloaded using the `zebra datasets create` command shown in the first section of this notebook. \n",
    "\n",
    "If the data has already be downloaded, the `zebra datasets inspect` command prints a summary of the dataset (or datsets), including a full list of the variables and some summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zebra datasets inspect --config-name=demo_nb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fcdca",
   "metadata": {},
   "source": [
    "The details of the dataset to be downloaded are defined in the datasets config file. The default datasets (those used by the `base` config) are `samp_sicsouth_osisaf_25k_2017_2019_24h_v1` and `samp_weathersouth_era5_0p5_2017_2019_24h_v1`. We will have a look at these in a bit more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/datasets/samp_sicsouth_osisaf_25k_2017_2019_24h_v1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b78dc9",
   "metadata": {},
   "source": [
    "This dataset contains sea ice concentration (SIC) data from the OSI SAF 25km dataset for the years 2017-2019, with a temporal resolution of 24 hours. The data is from the Southern Hemisphere. This download makes use of data preprocessing developed as part of the [IceNet repository](https://github.com/icenet-ai/icenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67961ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/datasets/samp_weathersouth_era5_0p5_2017_2019_24h_v1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e044a81",
   "metadata": {},
   "source": [
    "This dataset contains 50km resolution weather data from ERA5 for the years 2017-2019, with a temporal resolution of 24 hours. The data is from the Southern Hemisphere. The variables downloaded are those commonly used in sea ice forecasting, including temperature, u and v wind components, surface / sea level pressure, humidity and geopotential height. It also includes the cos and sin julian day to help the model learn seasonal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3893dbc",
   "metadata": {},
   "source": [
    "N.b. there are also a set of other configs in the `config/datasets` folder for different subsets of variables, different hemispheres, and different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b820fe",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca19dc5",
   "metadata": {},
   "source": [
    "Training a model is done using the `zebra train` command, as shown in section 1 of this notebook. There are a set of config files that specifically relate to the model training. For example the choice of model, the training parameters, the variable to predict, the dataset split and the logger to use. We will explore each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a30e8",
   "metadata": {},
   "source": [
    "### Model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85259c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/model/naive_unet_naive.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057f9d2",
   "metadata": {},
   "source": [
    "This config points to the specific python code used for each part of the model (i.e. the naive encoder, the UNet processor and the naive decoder). There are various parameters that can be set, such as the size of the latent space, the UNet kernel size and the number of start-out channels for the UNet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64548a",
   "metadata": {},
   "source": [
    "There are a set of possible model configs - each of these will have specific parameters (and their default values) that need to be set for those models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e25459",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ceeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/train/default.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c369e5",
   "metadata": {},
   "source": [
    "A set of training parameters are specified here, normally it is fine to just use the default values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c62bd",
   "metadata": {},
   "source": [
    "### Prediction parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fddef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/predict/osisaf-south.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848d7e4",
   "metadata": {},
   "source": [
    "The `predict` config files specify the variable to be predicted, as well as the number of historic days to include as input, and the number of days to forecast ahead. There are config files for predicting SIC in the northern or the southern hemisphere. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c8cec",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ed6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/split/sample_dataset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979c703",
   "metadata": {},
   "source": [
    "The dataset split specifies the number of batches to use for training. It also specifies how the datasets should be split in to training, validation and test sets, based on date ranges. These date shouldn't be altered, to ensure that all the different model runs are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a291d88",
   "metadata": {},
   "source": [
    "### Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/loggers/wandb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a9f2a",
   "metadata": {},
   "source": [
    "By default, the pipeline uses Weights & Biases to log the training and evaluation metrics. All runs are saved to the 'turing-seaice' project, as well as being saved locally in a `training` folder alongside the data. We don't log the model to W&B to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7531906",
   "metadata": {},
   "source": [
    "## Evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ce3a5",
   "metadata": {},
   "source": [
    "Training a model produces a set of artifacts, including one or more model checkpoint. The checkpoints can be used to evaluate the model performance using the `zebra evaluate` command (as shown in section 1 of this notebook). There are a couple of config files that specifically relate to the evaluation process within the `evaluate` folder (though several of the ones we have already explored are also relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c793f",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/evaluate/callbacks/plotting.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f86d43",
   "metadata": {},
   "source": [
    "By default, static maps and videos of the predicted forecasts of the output variable (normally SIC) are created for a set of examples from the test set. These are uploaded to Weights & Biases, and also saved locally. \n",
    "\n",
    "There are a lot of options for adjusting the model output plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d3f60",
   "metadata": {},
   "source": [
    "### Metric summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24648d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../ice_station_zebra/config/evaluate/callbacks/metric_summary.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fa93e",
   "metadata": {},
   "source": [
    "The default metric summary is based on average loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9cfee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Typically you should not change the values in the config files. If you want to change some of these options, the best way to do so is to create your own config file, similar to `demo_nb.yaml`, It's easiest to use the `base` config as the default, and then specify the parameters you want to change using `override` or by specifying the parameter in the config hierarchy. \n",
    "\n",
    "When you want to run code using that config file, you can specify it using the `--config-name` option in the CLI command. This will remove the need to specify the individual parameters in the command line each time. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a3a01",
   "metadata": {},
   "source": [
    "Hopefully this notebook has given you an overview of the Sea Ice forecasting pipeline, and shown you how you can use it flexibily to run different models and evaluate their performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
