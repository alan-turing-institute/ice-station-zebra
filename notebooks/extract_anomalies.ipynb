{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to synthetically create a non-gridded dataset of sea level pressure anomalies. \n",
    "\n",
    "To obtain sea level pressure anomalies we first need to get:\n",
    "1. Climatologies of mean sea level pressure (`msl`/`psl`) over the period 1979--2011 (following IceNet)\n",
    "2. `msl` data for the training period\n",
    "\n",
    "Anomalies are then calculated by subtracting the climatological mean for each calendar month from the `msl` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base path where data should be stored. This should be the same as what you use in your IceStationZebra config.\n",
    "base_path = \"/LOCAL/PATH/WHERE/YOU/WANT/TO/STORE/DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = (Path(base_path) / \"data\" / \"notebooks\" / \"extract_anomalies\").resolve()\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "current_directory = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download monthly means (1979-2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download from the CDS dataset [ERA5 monthly averaged data on single levels from 1940 to present](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=download). We'll use the CDS API directly, as it is not currently clear how to do so with Anemoi. To use the CDS API you need to have the `cdsapi` package installed and have a `.cdsapirc` file set up under your `$HOME`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download only data for the Southern hemisphere. You can do the same for the Northern hemisphere by changing the `area` key to `[90, -180, 0, 180]`, or leave out the key altogether to get global data. The data is saved in `msl_monthly_south_1979_2020.nc`. Provide the full path if you want it saved under another directory, e.g. `../data/msl_monthly_south_1979_2020.nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "\n",
    "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_reanalysis\"],\n",
    "    \"variable\": [\"mean_sea_level_pressure\"],\n",
    "    \"year\": [\n",
    "        \"1979\",\n",
    "        \"1980\",\n",
    "        \"1981\",\n",
    "        \"1982\",\n",
    "        \"1983\",\n",
    "        \"1984\",\n",
    "        \"1985\",\n",
    "        \"1986\",\n",
    "        \"1987\",\n",
    "        \"1988\",\n",
    "        \"1989\",\n",
    "        \"1990\",\n",
    "        \"1991\",\n",
    "        \"1992\",\n",
    "        \"1993\",\n",
    "        \"1994\",\n",
    "        \"1995\",\n",
    "        \"1996\",\n",
    "        \"1997\",\n",
    "        \"1998\",\n",
    "        \"1999\",\n",
    "        \"2000\",\n",
    "        \"2001\",\n",
    "        \"2002\",\n",
    "        \"2003\",\n",
    "        \"2004\",\n",
    "        \"2005\",\n",
    "        \"2006\",\n",
    "        \"2007\",\n",
    "        \"2008\",\n",
    "        \"2009\",\n",
    "        \"2010\",\n",
    "        \"2011\",\n",
    "        \"2012\",\n",
    "        \"2013\",\n",
    "        \"2014\",\n",
    "        \"2015\",\n",
    "        \"2016\",\n",
    "        \"2017\",\n",
    "        \"2018\",\n",
    "        \"2019\",\n",
    "        \"2020\",\n",
    "    ],\n",
    "    \"month\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "    \"area\": [0, -180, -90, 180],\n",
    "}\n",
    "\n",
    "os.chdir(data_path)\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download(\"msl_monthly_south_1979_2020.nc\")\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download daily data (2014-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the daily `msl` data from ERA5. This data could be downloaded using Anemoi, but it would be in a different resolution and gridding system (o320) than the data downloaded directly using the CDS API or `download_toolbox`, which means you'll have to regrid it. We'll use the `download_era5` command from the `download_toolbox` CLI, which will download ERA5 daily data on a 0.25 x 0.25 degree grid, like the monthly averages downloaded above. You can also download this using the CDS API directly, but `download_toolbox` simply seems a lot faster. You can call the command from this notebook with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "os.chdir(data_path)\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"download_era5\",\n",
    "        \"-f\",\n",
    "        \"DAY\",\n",
    "        \"-o\",\n",
    "        \"MONTH\",\n",
    "        \"south\",\n",
    "        \"2014-01-01\",\n",
    "        \"2020-12-31\",\n",
    "        \"psl\",\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB: this will save the data under `${data_path}/data/era5/psl/south/*nc`, in monthly grouped files. But it will also duplicate them under `${data_path}/data/era5/day/south/psl/*nc`. Apparently, there's supposed to be a \"delete cache\" option in the download_toolbox, which I couldn't find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(data_path / \"msl_monthly_south_1979_2020.nc\", engine=\"h5netcdf\")\n",
    "print(ds)\n",
    "lat_step = abs(ds.latitude[1].item() - ds.latitude[0].item())\n",
    "lon_step = abs(ds.longitude[1].item() - ds.longitude[0].item())\n",
    "print(f\"Resolution: {lat_step} x {lon_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "times = pd.to_datetime(ds.valid_time.values)\n",
    "print(f\"Start: {times.min()}\")\n",
    "print(f\"End: {times.max()}\")\n",
    "print(f\"Number of months: {len(times)}\")\n",
    "expected = pd.date_range(\"1979-01-01\", \"2020-12-01\", freq=\"MS\")\n",
    "missing = expected.difference(times)\n",
    "print(f\"Missing {len(missing)} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(data_path / \"data/cds/psl/south/201405.nc\", engine=\"h5netcdf\")\n",
    "print(ds)\n",
    "lat_step = abs(ds.latitude[1].item() - ds.latitude[0].item())\n",
    "lon_step = abs(ds.longitude[1].item() - ds.longitude[0].item())\n",
    "print(f\"Resolution: {lat_step} x {lon_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(data_path / \"msl_monthly_south_1979_2020.nc\", engine=\"h5netcdf\")\n",
    "# Take the mean across years for each calendar month. Shape: (12, lat, lon)\n",
    "clim = ds[\"msl\"].groupby(\"valid_time.month\").mean(dim=\"valid_time\")\n",
    "len(clim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim.to_dataset(name=\"msl\").to_netcdf(data_path / \"data/climatology_12_month.nc\", engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = xr.open_dataset(data_path / \"data/climatology_12_month.nc\", engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# open clim file if not already loaded from previous\n",
    "clim = xr.open_dataset(data_path / \"data/climatology_12_month.nc\", engine=\"h5netcdf\")\n",
    "\n",
    "# Daily files folder\n",
    "input_folder = data_path / \"data/cds/psl/south/\"\n",
    "output_folder = data_path / \"data/era5/anom_psl/south/\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of daily .nc files\n",
    "daily_files = sorted(glob.glob(os.path.join(input_folder, \"*.nc\")))\n",
    "\n",
    "for f in daily_files:\n",
    "    print(f\"Processing {f}\")\n",
    "    ds_daily = xr.open_dataset(f, engine=\"h5netcdf\")\n",
    "\n",
    "    # Extract month from the first time value\n",
    "    month = ds_daily[\"time\"].dt.month[0].item()\n",
    "\n",
    "    # Select the corresponding climatology slice\n",
    "    clim_month = clim.sel(month=month)\n",
    "\n",
    "    # Subtract climatology from daily values\n",
    "    anomaly = ds_daily[\"psl\"] - clim_month[\"msl\"]\n",
    "\n",
    "    # Preserve original filename, save under new folder\n",
    "    filename = os.path.basename(f)\n",
    "    out_file = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Save anomaly\n",
    "    anomaly.to_dataset(name=\"psl_anom\").to_netcdf(out_file, engine=\"h5netcdf\")\n",
    "\n",
    "    print(f\"Saved {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_anom = xr.open_dataset(data_path / \"data/era5/anom_psl/south/201401.nc\", engine=\"h5netcdf\")\n",
    "ds_mls = xr.open_dataset(data_path / \"data/cds/psl/south/201401.nc\", engine=\"h5netcdf\")\n",
    "\n",
    "psl_anom = ds_anom[\"psl_anom\"].isel(time=0) / 100  # Convert Pa → hPa\n",
    "ds_mls = ds_mls[\"psl\"].isel(time=0) / 100  # Convert Pa → hPa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antarctic_extent = [-180, 180, -90, -60]\n",
    "arctic_extent = [-180, 180, 60, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean\n",
    "\n",
    "\n",
    "def plot_msl(msl_data, title):\n",
    "    is_arctic = msl_data.latitude.mean().item() > 0\n",
    "    proj = ccrs.NorthPolarStereo() if is_arctic else ccrs.SouthPolarStereo()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "\n",
    "    # Set appropriate extent for polar views\n",
    "    if is_arctic:\n",
    "        ax.set_extent(arctic_extent, crs=ccrs.PlateCarree())\n",
    "    else:\n",
    "        ax.set_extent(antarctic_extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Basemap features\n",
    "    ax.coastlines(resolution=\"50m\", linewidth=1)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.LAND, facecolor=\"lightgray\")\n",
    "    # ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    # Pressure levels\n",
    "    levels = np.linspace(msl_data.min().item(), msl_data.max().item(), 30)\n",
    "    cs = ax.contourf(\n",
    "        msl_data.longitude,\n",
    "        msl_data.latitude,\n",
    "        msl_data,\n",
    "        levels=levels,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmocean.cm.diff,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "\n",
    "    # Colorbar\n",
    "    plt.colorbar(cs, orientation=\"horizontal\", pad=0.05, aspect=50, label=\"MSLP (hPa)\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Apply tight layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_msl(ds_mls, title=\"MSLP (Antarctic)\")\n",
    "plot_msl(psl_anom, title=\"MSLP Anomalies (Antarctic)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
